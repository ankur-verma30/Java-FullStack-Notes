ğŸ”· DevOps
DevOps (Development + Operations) is a set of practices that combines software development (Dev) and IT operations (Ops) to:
â¤ Increase software delivery speed using Automation
â¤ Improve product quality
â¤ Enable continuous integration and deployment (CI/CD)
â¤ Continuous Monitoring and logging
â¤ Continuous Testing
DevOps is a culture and process that helps teams build, test, and release software faster and more reliably through automation and collaboration.

ğŸ”· Why Devops?
Because it solves the traditional gap between developers and operations teams.

ğŸ”‘ Key Reasons:
âœ”ï¸ Faster Delivery: Automates build, test, and deployment â†’ delivers updates quickly.
âœ”ï¸ Better Collaboration: Breaks silos between dev & ops â†’ teams work together efficiently.
âœ”ï¸ Higher Quality: Continuous testing & monitoring â†’ fewer bugs in production.
âœ”ï¸ Scalability: Easier to handle large, complex systems through automation tools.
âœ”ï¸ Reliability: Continuous integration and deployment ensure stable, consistent releases.
âœ”ï¸ Quick Recovery: Issues are detected and fixed faster with monitoring and rollback features.
âœ”ï¸ Customer Satisfaction: Frequent updates and faster fixes â†’ happier users.


ğŸ”· Software Development Lifecycle(SDLC) 
SDLC is the process used to design, develop, test, and deploy software systematically and efficiently.

ğŸ§© Phases of SDLC:
âœ”ï¸ Requirement Analysis(Planning and Defining)
â¤ Understand what the client/user needs.
â¤ Gather and docu ment functional & non-functional requirements.
â¤ Output: Software Requirement Specification (SRS)

âœ”ï¸ System Design
â¤ Plan architecture, database, UI, and technology stack.
â¤ Output: Design Documents, Wireframes

âš¡ DevOps Centric Phases
âœ”ï¸ Implementation (Coding)
â¤ Developers write code based on the design.
â¤ Follow coding standards and version control.

âœ”ï¸ Testing
â¤ Verify that the software works correctly and meets requirements.
â¤ Includes unit testing, integration testing, and system testing.

âœ”ï¸ Deployment
â¤ Release the software to production (end users).
â¤ Can be manual or automated (DevOps CI/CD).

âœ”ï¸ Maintenance
â¤ Fix bugs, update features, and ensure system reliability.

ğŸ” Popular SDLC Models:
â¤ Waterfall Model: Linear, phase-by-phase.
â¤ Agile Model: Iterative and flexible.
â¤ Spiral Model: Combines design + prototyping.
â¤ V-Model: Verification and validation in parallel.


ğŸ”· Roles in Company
âœ”ï¸ Business Analyst: Gathers and documents customer requierments.
âœ”ï¸ Product Manager: Defines the vision, goals, and priorities
âœ”ï¸ Product Owner: Manages baclong and converts vision into actionable stories
âœ”ï¸ UI/UX Designer: Designs user interface and user experience.
âœ”ï¸ Software Architect: Designs technical system structure and frameworks.
âœ”ï¸ Developers: Build the actual product(UI, APIs, databases).
âœ”ï¸ DBA: Designs and manages the database.
âœ”ï¸ Security Engineer: Ensures product and infrasturcture security.
âœ”ï¸ QA Engineer: Tests product quality and performance.
âœ”ï¸ DevOps Engineer: Builds CI/CD pipelines and manages environments.
âœ”ï¸ Release Manager: Plans and manages releases.
âœ”ï¸ SRE: Ensures uptime, performance, and reliability post-release.
âœ”ï¸ Technical Writer: Creates documentation for users and developers.

ğŸ”· Virtual Machines (VM)
â¤ A Virtual Machine is a software-based computer that runs inside another physical computer.
â¤ It behaves just like a real machine (has its own CPU, memory, disk, OS), but it's virtual â€” created using virtualization software.
â¤ A VM is like a computer inside a computer.

ğŸŒ What is a Server?
A server is a computer that provides resources or services (like websites, databases, or files) to other computers (called clients) over a network.

âœ… Example:
âœ”ï¸ A web server hosts websites.
âœ”ï¸ A database server stores and manages data.

âš™ï¸ Physical vs Virtual Servers
| Feature            | **Physical Server**                      | **Virtual Server (VM)**                                        |
| ------------------ | ---------------------------------------- | -------------------------------------------------------------- |
| **Definition**     | Actual hardware machine                  | Software-based machine running on a physical host              |
| **Resource Usage** | Dedicated (not shared)                   | Shared among multiple VMs                                      |
| **Scalability**    | Difficult (requires new hardware)        | Easy (create new VMs quickly)                                  |
| **Cost**           | Expensive                                | Cost-effective                                                 |
| **Isolation**      | One OS per server                        | Multiple OSs can run on one physical machine                   |
| **Example**        | A Dell PowerEdge server in a data center | Several Ubuntu/Windows VMs running inside VMware or VirtualBox |


ğŸ”„ Hypervisor
A Hypervisor is software that creates and manages virtual machines by sharing physical resources (CPU, RAM, disk) between them. 
ğŸ§© Types of Hypervisors:
ğŸ‘‰ Type 1 â€“ Bare Metal Hypervisor:
â¤ Runs directly on hardware (no host OS).
âœ… Examples: VMware ESXi, Microsoft Hyper-V, KVM

ğŸ‘‰ Type 2 â€“ Hosted Hypervisor:
â¤ Runs on top of a host OS (like an app).
âœ… Examples: Oracle VirtualBox, VMware Workstation

ğŸŒ Real-World Examples:
âœ”ï¸ Cloud Providers:
â¤  AWS EC2, Google Cloud Compute Engine, Azure VMs â€” all use virtualization to create VMs on demand.

âœ”ï¸ Development & Testing:
â¤  Developers use VMs to test software in multiple OS environments.

âœ”ï¸ Server Consolidation:
â¤  Companies run multiple VMs (web server, DB server, mail server) on a single physical server to save costs.


â“ How to Automate the creation of Virtual Machines on any Cloud Platform using API?
Automation means you donâ€™t manually click buttons to create a VM â€” instead, you use scripts or tools to create and manage VMs automatically.

1ï¸âƒ£ AWS CLI (Command Line Interface)
â¤ A command-line tool to interact directly with AWS services.
â¤ Executes imperative commands (step-by-step actions).

âœ… Example:
aws ec2 run-instances --image-id ami-0abcdef --instance-type t2.micro

ğŸ”„ Use case:
â¤ Quick manual automation or scripting.
â¤ Good for ad-hoc tasks, like creating or stopping instances.
âš™ï¸ Level: Low-level (manual commands)

2ï¸âƒ£ AWS SDK / API
â¤ Programmatic access to AWS services using code.
â¤ Available for languages like Python (Boto3), JavaScript, Java, etc.

âœ… Example (Python Boto3):
import boto3
ec2 = boto3.client('ec2')
ec2.run_instances(ImageId='ami-0abcdef', InstanceType='t2.micro', MinCount=1, MaxCount=1)

ğŸ”„ Use case:
â¤ When you want to integrate AWS operations inside an app or custom automation scripts.
âš™ï¸ Level: Low-level (programmatic control)

3ï¸âƒ£ AWS CloudFormation (CFT)
â¤ Infrastructure as Code (IaC) tool native to AWS.
â¤ Uses YAML/JSON templates to define and deploy AWS resources.

âœ… Example (YAML):
Resources:
  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: ami-0abcdef
      InstanceType: t2.micro

ğŸ”„ Use case:
â¤ Automate AWS infrastructure provisioning in a declarative way.
â¤ Used for repeatable deployments (same infra across environments).
âš™ï¸ Level: Declarative IaC (AWS-only)

4ï¸âƒ£ Terraform
â¤ Multi-cloud Infrastructure as Code tool by HashiCorp.
â¤ Works with AWS, Azure, GCP, etc.
â¤ Uses HCL (HashiCorp Configuration Language).

âœ… Example (HCL):
provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "example" {
  ami           = "ami-0abcdef"
  instance_type = "t2.micro"
}

ğŸ”„ Use case:
â¤ Cross-cloud automation
â¤ Version-controlled infrastructure
â¤ More flexible than CloudFormation (modular, reusable, portable).
âš™ï¸ Level: Declarative IaC (multi-cloud)

ğŸ’¡ 5ï¸âƒ£ AWS CDK (Cloud Development Kit)
â¤ High-level Infrastructure as Code using real programming languages (TypeScript, Python, Java, etc.)
â¤ Converts your code â†’ CloudFormation under the hood.

âœ… Example (TypeScript):
import * as ec2 from 'aws-cdk-lib/aws-ec2';
import { Stack } from 'aws-cdk-lib';

class MyStack extends Stack {
  constructor(scope, id, props) {
    super(scope, id, props);
    new ec2.Instance(this, 'MyInstance', {
      instanceType: new ec2.InstanceType('t2.micro'),
      machineImage: ec2.MachineImage.latestAmazonLinux(),
    });
  }
}

ğŸ”„ Use case:
â¤ Developers who prefer real code over YAML.
â¤ Allows loops, conditions, functions in infra definitions.
âš™ï¸ Level: High-level IaC (code-driven)

| Tool                     | Type            | Scope       | Language         | Style                    | Use Case                     |
| ------------------------ | --------------- | ----------- | ---------------- | ------------------------ | ---------------------------- |
| **AWS CLI**              | Command line    | AWS only    | Shell            | Imperative               | Quick manual automation      |
| **AWS SDK/API**          | Programming API | AWS only    | Python, JS, etc. | Imperative               | Embed AWS ops in code        |
| **CloudFormation (CFT)** | IaC             | AWS only    | YAML/JSON        | Declarative              | AWS-native infra automation  |
| **Terraform**            | IaC             | Multi-cloud | HCL              | Declarative              | Cross-cloud infra automation |
| **AWS CDK**              | IaC             | AWS only    | Python/TS/Java   | Declarative (code-based) | Code-native infra definition |


ğŸ”· Connecting to EC2 Instance
1ï¸âƒ£ From AWS Management Console (UI)
Go to EC2 Dashboard â†’ Instances.
Select your running instance.
Click Connect at the top.
Choose EC2 Instance Connect or Session Manager.
Click Connect to open a browser-based terminal session.

âœ… Advantage:
No local setup required.
Secure and easy for quick access.

ğŸ’» Method 2: From CLI (Git Bash / Terminal)
Used when you want to connect directly from your local machine.

1ï¸âƒ£ Set file permissions for keypair (.pem file):
ğŸ”— chmod 400 my-key-pair.pem

â¤ This ensures that only you (the file owner) can read the key file â€” a security requirement by SSH.

2ï¸âƒ£ Connect using SSH:
ğŸ”— ssh -i "my-key-pair.pem" ec2-user@ec2-3-109-59-6.ap-south-1.compute.amazonaws.com

âœ”ï¸ -i â†’ specifies the private key file
âœ”ï¸ ec2-user â†’ default username for Amazon Linux
âœ”ï¸ The public DNS name is unique for your instance

ğŸ”„ Advantage:
â¤ Direct access from terminal
â¤ Useful for automation, DevOps scripts, and deployments

ğŸ” Stopping and Terminating EC2 Instances
ğŸ“´ Stop Instance
â¤ Temporarily shuts down the instance.
â¤ Data on EBS volume is retained.
â¤ You can start it again later.

âŒ Terminate Instance
â¤ Permanently deletes the instance.
â¤ All data on instance store volumes is lost.
â¤ Cannot be restarted once terminated.

âš¡ Note:
Always stop first, then terminate only if not needed further.


ğŸ”· AWS CLI
AWS CLI is a command-line tool that allows you to interact with AWS services directly from your terminal instead of using the AWS Management Console (UI).

ğŸ”„ Key Features:
1ï¸âƒ£ Automate AWS tasks using commands or scripts
2ï¸âƒ£ Manage EC2 instances, S3 buckets, IAM users, etc.
3ï¸âƒ£ Integrates with Shell Scripting and CI/CD pipelines

ğŸ”„ Setup Steps:
1ï¸âƒ£ Install AWS CLI:
ğŸ”— Download from https://aws.amazon.com/cli/

2ï¸âƒ£ Configure credentials:
ğŸ”— aws configure

ğŸ‘‰ Enter:
  âœ”ï¸ AWS Access Key ID
  âœ”ï¸ AWS Secret Access Key
  âœ”ï¸ Default region name (e.g., ap-south-1)
  âœ”ï¸ Default output format (json, text, or table)

3ï¸âƒ£  Verify installation:
ğŸ”— aws --version

âœ… Example Commands:
ğŸ”— aws ec2 describe-instances
ğŸ”— aws s3 ls
ğŸ”— aws ec2 stop-instances --instance-ids i-0abcd1234ef5678gh

ğŸ“ AWS CLI Command Reference: https://docs.aws.amazon.com/cli/latest/userguide/cli-services-ec2-instances.html
ğŸ“ AWS Cloud Formation Template Example: https://github.com/aws-cloudformation/aws-cloudformation-templates
ğŸ“ AWS Python Boto3 Example: https://gist.github.com/mda590/679aba60ca03699d5b12a32314debdc0


ğŸ”· Version Control System (VCS)
A Version Control System tracks changes in code/files, enables collaboration, and keeps history of all modifications.

Two major types:
1ï¸âƒ£ Centralized VCS (CVCS)
2ï¸âƒ£ Decentralized / Distributed VCS (DVCS)

ğŸ”µ Centralized Version Control System (CVCS)
A centralized VCS has a single central server that stores the entire codebase and its history.
Developers connect to this server to commit, update, and retrieve code.

âœ… Examples
â¤ SVN (Subversion)
â¤ CVS
â¤ Perforce

ğŸ”„ How it works
â¤ One central repository lives on a server.
â¤ Developers only keep a working copy, not full history.
â¤ If the server is down â†’ no commits, no history access.

ğŸ”„ Advantages
â¤ Simple to understand
â¤ Centralized control (good for enterprises)
â¤ Easy to maintain permissions

ğŸ”„ Disadvantages
â¤ Single point of failure (Server crash = work stops)
â¤ Slow operations (network-dependent)
â¤ Limited offline work
â¤ Server load is high

ğŸŸ¢  Decentralized / Distributed Version Control System (DVCS)
A distributed VCS gives every developer a complete copy of the entire repository, including full history.

âœ… Examples
âœ”ï¸ Git (Most widely used today)
âœ”ï¸ Mercurial
âœ”ï¸ Bazaar

ğŸ”„ How it works
â¤ Every developer has a local repo (local copy of full history).
â¤ Commits, branching, merging happen locally.
â¤ Pushing/pulling happens with a remote like GitHub/GitLab.

ğŸ”„ Advantages
â¤ No single point of failure
â¤ Almost everything can be done offline
â¤ Fast operations (local)
â¤ Easier branching & merging
â¤ Highly scalable for large teams

ğŸ”„ Disadvantages
â¤ Slightly more complex to understand
â¤ Requires more local storage (full history stored locally)


ğŸ”· Git Branching Strategy
A Git branching strategy defines how teams create, use, and merge branches in a project.
It ensures:
â¤ Clean and predictable workflow
â¤ Easy collaboration
â¤ Safe deployment
â¤ Reduced merge conflicts
â¤ Stable production releases

ğŸ”¥ Why Do We Need a Branching Strategy?
â¤ Because without a strategy:
â¤ Developers overwrite each otherâ€™s work
â¤ Releases become unstable
â¤ Hard to review code
â¤ Hotfixes break existing code
â¤ Production gets messy

Branching strategies bring discipline and structure to teamwork.

ğŸ§± Popular Git Branching Strategies
1ï¸âƒ£ Git Flow (Most Traditional & Full-featured)
Used in large, release-focused projects.

ğŸ”— Main Branches
  âœ”ï¸ main â†’ always production-ready
  âœ”ï¸ develop â†’ integration branch for features

ğŸ”— Supporting Branches
  âœ”ï¸ feature/* â†’ new features
  âœ”ï¸ release/* â†’ preparing new release
  âœ”ï¸ hotfix/* â†’ emergency fixes in production

ğŸ”„ Flow Summary
feature â†’ develop â†’ release â†’ main â†’ tag version
                      â†‘
                 hotfix â†’ main

âš¡ Pros
â¤ Great for large teams
â¤ Very structured
â¤ Supports multiple releases

âŒ Cons
â¤ Too heavy for small/fast-moving teams

2ï¸âƒ£ GitHub Flow (Simple & Best for Continuous Deployment)
Used in modern web companies (GitHub, Facebook, etc.)

ğŸ”„ Structure
â¤ Only one main branch: main
â¤ New work â†’ feature-branch
â¤ After work is done â†’ Open Pull Request â†’ Review â†’ Merge to main
â¤ Deployment happens after merge

âš¡ Pros
â¤ Very simple
â¤ Works great with CI/CD
â¤ Fast iterations

âŒCons
â¤ No separate develop or release branches
â¤ Not suitable for complex release cycles

3ï¸âƒ£ GitLab Flow (Hybrid of Git Flow + GitHub Flow)
Mixes environment branches (like staging, production).

ğŸ”„ Common Flow
feature â†’ main â†’ staging â†’ production

âš¡Pro
â¤ Supports environments (QA, staging).
â¤ Balanced between complexity and simplicity

âŒ Cons
â¤ Slightly more complex than GitHub Flow

4ï¸âƒ£ Trunk-Based Development (Used by Google, Netflix)
Very fast and extreme Agile.

ğŸ”„ Rules
  âœ”ï¸ Only one main branch â†’ main
  âœ”ï¸ Developers create short-lived feature branches
  âœ”ï¸ Merge into main daily or several times a day
  âœ”ï¸ Requires feature flags to hide incomplete features

âš¡ Pros
â¤ Very fast releases
â¤ Minimal merge conflicts
â¤ Perfect for DevOps

âŒ Cons
â¤ Requires high discipline
â¤ Needs strong automation & CI/CD

â­ Which Branching Strategy Should You Use? (Interview Answer)
â¤ Small teams / startups: GitHub Flow
â¤ Large enterprise projects: Git Flow
â¤ CI/CD-heavy teams (DevOps): Trunk-Based Development
â¤ Teams with environments (QA, staging): GitLab Flow


ğŸ”· AWS services For DevOps
1ï¸âƒ£ EC2 (Elastic Compute Cloud)
2ï¸âƒ£ VPC (Virtual Private Cloud)
3ï¸âƒ£ EBS (Elastic Block Store)
4ï¸âƒ£ S3 (Simple Storage Service)
5ï¸âƒ£ IAM (Identity and Access Management)
6ï¸âƒ£ CloudWatch (Monitoring and Logging)
7ï¸âƒ£ Lambda (Serverless Computing) 
8ï¸âƒ£ CodeBuild (Build Service)
  1ï¸âƒ£ AWS CodePipeline (CI/CD Service)
  2ï¸âƒ£ AWS CodeBuild (Build Service)
  3ï¸âƒ£ AWS CodeDeploy (Deployment Service)
9ï¸âƒ£ AWS Configuration Manager (AWS Config)
1ï¸âƒ£0ï¸âƒ£ Billing and Cost Management
1ï¸âƒ£1ï¸âƒ£ AWS KMS (Key Management Service)
1ï¸âƒ£2ï¸âƒ£ AWS CloudTrail (Audit and Compliance)
1ï¸âƒ£3ï¸âƒ£ AWS EKS (Elastic Kubernetes Service)
1ï¸âƒ£4ï¸âƒ£ Fargate, ECS 
1ï¸âƒ£5ï¸âƒ£ ELK Stack (Elasticsearch, Logstash, Kibana)


ğŸ”·Configuration Management With Ansible
Configuration Management (CM) is the process of automating the setup, configuration, and management of servers so they remain consistent across environments.

ğŸ”„ Why do we need CM?
âœ”ï¸ Avoid manual configuration
âœ”ï¸ Ensure consistency across Dev, QA, Prod
âœ”ï¸ Reduce human errors
âœ”ï¸ Automate deployments & scaling
âœ”ï¸ Improve reliability for infrastructure

ğŸŒŸ Configuration Management with Ansible
Ansible is a simple, agentless, open-source configuration management and automation tool.

ğŸ”„ It automates:
â¤  Server configuration
â¤  Application deployment
â¤  Orchestration
â¤  Cloud provisioning

ğŸ” Key Features
â¤ Agentless (No software installation on client machines)
â¤ Uses SSH for communication
â¤ YAML-based Playbooks (simple to read and write)
â¤ Idempotent (same script can run multiple times without breaking anything)


ğŸ”„ Components of Ansible
1ï¸âƒ£ Control Node
  âœ”ï¸ Where Ansible is installed
  âœ”ï¸ Executes Playbooks
2ï¸âƒ£ Managed Nodes
  âœ”ï¸ Target servers
  âœ”ï¸ No agent required
3ï¸âƒ£ Inventory File
  âœ”ï¸ List of hosts (IP or domain)
4ï¸âƒ£ Playbooks (YAML)
  âœ”ï¸ Written instructions for tasks

âœ… Example Inventory:
[webservers]
192.168.1.10
192.168.1.11

âœ… Example Playbook:
- hosts: webservers
  tasks:
    - name: install nginx
      apt:
        name: nginx
        state: present


ğŸ†š Puppet vs Ansible
| Feature              | **Puppet**                     | **Ansible**                                     |
| -------------------- | ------------------------------ | ----------------------------------------------- |
| **Type**             | Configuration Management       | Configuration Management + Orchestration        |
| **Language**         | DSL (Domain Specific Language) | YAML (Easy)                                     |
| **Agent**            | **Agent-based**                | **Agentless**                                   |
| **Communication**    | Client-Agent Architecture      | SSH / WinRM                                     |
| **Setup Complexity** | High                           | Very Easy                                       |
| **Learning Curve**   | Steep                          | Beginner-friendly                               |
| **Ease of Use**      | Medium                         | Very High                                       |
| **Speed**            | Slower initial setup           | Faster setup + execution                        |
| **Push/Pull Model**  | Pull (Clients pull configs)    | Push (Control pushes configs)                   |
| **Best For**         | Large enterprise environments  | Small to medium teams; also used in enterprises |
| **Scaling**          | Very powerful                  | Scales decently                                 |

ğŸ” Puppet
â¤ Follows a clientâ€“server model
â¤ Each node runs a Puppet Agent
â¤ Agents connect to Puppet Master to fetch configuration
â¤ Uses its own scripting language (Puppet DSL)
â¤ Suitable for large enterprises with complex infrastructures

ğŸ” Ansible
â¤ No agent required on target machines
â¤ Uses SSH, which is built in everywhere
â¤ Easier to start & maintain
â¤ Playbooks written in YAML (very readable)
â¤ Very popular in DevOps & Cloud automation
â¤ Faster to adopt and integrate with CI/CD


ğŸ”· How to Setup Ansible in EC2 instance
ğŸŸ¦ Step 1: Launch EC2 Instances 
You need at least 2 EC2 instances:

1ï¸âƒ£ Control Node
Ansible will be installed here
OS: Ubuntu is recommended

2ï¸âƒ£ Managed Node(s)
Ansible will configure these
Can be Ubuntu / Amazon Linux / CentOS
You can have 1 or more managed nodes.

ğŸŸ¦ Step 2: Connect to the Control Node
ğŸ‘‰ SSH into the control node:
ğŸ”— ssh -i your-key.pem ubuntu@public-ip

ğŸ‘‰ Update system:
ğŸ”— sudo apt update -y

ğŸŸ¢ Step 3: Install Ansible on Control Node
ğŸ‘‰ For Ubuntu:
ğŸ”— sudo apt install ansible -y

ğŸ‘‰ Check version:
ğŸ”— ansible --version

use command:
ssh-keygen
generates public key in path /home/ubuntu/.ssh/id_ed25519.pub and private key in path /home/ubuntu/.ssh/id_ed25519

ğŸŸ¦ Step 4: Create an Inventory File
Ansible uses an inventory to know which servers to manage.

ğŸ‘‰ Create a directory:
ğŸ”— mkdir ansible-setup
ğŸ”— cd ansible-setup

ğŸ‘‰ Create inventory file:
ğŸ”— nano inventory

Add your managed node IPs:
[webservers]
âœ”ï¸ ec2-user@<managed-node-1-public-ip>
âœ”ï¸ ec2-user@<managed-node-2-public-ip>
â€» If using Ubuntu as managed nodes, replace ec2-user with ubuntu.

ğŸŸ¢ Step 5: Copy SSH Key to Control Node
You must move your .pem key to the control node:

ğŸ‘‰ On your laptop:
ğŸ”— scp -i your-key.pem your-key.pem ubuntu@control-node-ip:/home/ubuntu/

ğŸ‘‰ Inside control node:
ğŸ”— chmod 400 your-key.pem

ğŸŸ¦ Step 6: Test Connection from Ansible to Managed Node

ğŸ‘‰ Run:
ğŸ”— ansible -i inventory webservers -m ping --private-key your-key.pem

ğŸ‘‰ Expected output if successful:
managed-node-ip | SUCCESS => {
    "changed": false,
    "ping": "pong"
}

ğŸ”· Ansible Adhoc Commands
ğŸ”— ansible -i inventory all -m "shell" -a "touch devopsclass.txt" â†’ create a file through ansible in the target ec2 instance  

ğŸ‘‰ For specific groups we can mention the gruop name
ğŸ”— ansible -i inventory webservers -m "shell" -a "touch devopsclass.txt"

ğŸ”— ansible
Runs the Ansible adhoc command.

ğŸ”— -i inventory
Tells Ansible to use the file named inventory as the list of target hosts.

âœ… Example inventory file:
[servers]
3.110.23.41
13.232.90.77

ğŸ”— all
This means run on all hosts defined inside the inventory file.
can be changed to any group name you want

ğŸ”— -m command
â¤ You are using the command module, which allows running Linux shell commands on the remote machine.
â¤ This module executes raw commands just like running them via SSH.

ğŸ”— -a "touch devopsclass.txt"
The argument (-a) sent to the module:
Ansible will run:

ğŸ”— touch devopsclass.txt
on the remote EC2 instance.
ğŸ‘‰ This command creates an empty file named devopsclass.txt in the userâ€™s home directory on the target system.

ğŸ”„ Final Meaning of the Entire Command
Create a file named devopsclass.txt on all EC2 instances listed in the inventory file using Ansible.

âœ… Example Output
You might see:
3.110.23.41 | CHANGED | rc=0 >>
13.232.90.77 | CHANGED | rc=0 >>

ğŸŸ¢ Step 7: Create a Playbook (Optional but Recommended)
Example: Install NGINX on nodes.

ğŸ‘‰ Create a playbook:
ğŸ”— nano install-nginx.yml

Standard ways to create playbooks files
1ï¸âƒ£ add 3 hyphens to the top of the file to identify it is a yml file


ğŸ‘‰ Add:
ğŸ”—- name : Install and Start nginx   â†’  playbook number 1
 - hosts: webservers
  become: root     # what is the privledge provided to the ansible 
  tasks:  
  1ï¸âƒ£  - name:  Install Nginx     â†’  task 1
      apt:           â†’  more generic 
        name: nginx
        state: present   #present means install the package 
âœ”ï¸ Equivalent command is:
      shell: apt install nginx  â†’  Less recommended

2ï¸âƒ£   - name:  Start Nginx       â†’  task 2
      service:
        name: nginx
        state: started
      
ğŸ‘‰ Run playbook:
ğŸ”— ansible-playbook -i inventory install-nginx.yml  

ğŸŸ¦ Step 8: Verify on Managed Nodes
ğŸ‘‰ SSH into managed node:
ğŸ”— ssh -i your-key.pem ubuntu@managed-node-ip

ğŸ‘‰ Check Nginx:
ğŸ”— systemctl status nginx


ğŸ”· Ansible Roles 
ğŸ”¹ Why Ansible Roles?
In real-world infra automation, playbooks become large & difficult to manage.

âœ…Example:
ğŸ‘‰ Create 3 EC2 instances using Terraform
ğŸ‘‰ Configure 1 as master and 2 as workers using Ansible

Writing all logic (master + worker tasks) inside a single playbook becomes:
âœ”ï¸ Cumbersome
âœ”ï¸ Hard to maintain
âœ”ï¸ Not scalable
Ansible Roles provide a clean, modular structure for organizing complex automation.

â¤ A role is a predefined directory structure that logically groups:
âœ”ï¸ tasks
âœ”ï¸ variables
âœ”ï¸ templates
âœ”ï¸ files
âœ”ï¸ handlers
âœ”ï¸ metadata
ğŸ‘‰ Roles allow you to reuse and share automation code easily.
ğŸ‘‰ Encouraged best practice when automating large, multi-component environments (like Kubernetes clusters).

ğŸ”„ Creating a Role
Command:
ğŸ”— ansible-galaxy role init Kubernetes
Creates a folder named Kubernetes with a standard structure.

ğŸ”¹ Role Directory Structure (with meaning)
âœ”ï¸ tasks/
â¤ Contains the main automation logic.
â¤ Equivalent to writing tasks inside a playbook.
âœ… Example: installing Docker, configuring Kubeadm, etc.

âœ”ï¸ handlers/
â¤ Triggered when tasks report a change.
â¤ Used to restart services, reload daemons, etc.

âœ”ï¸ files/
Stores static files such as:
  âš¡ certs
  âš¡ index.html
  âš¡ config files that need to be copied as-is

âœ”ï¸ templates/
â¤ Stores Jinja2 templates (*.j2).
â¤ Used for dynamic configuration files (config with variables).

âœ”ï¸ vars/
â¤ Stores variables that rarely change.
â¤ Higher priority than defaults.

âœ”ï¸ defaults/
â¤ Stores default variable values.
â¤ Lowest precedence â†’ easiest to override.

âœ”ï¸ meta/
Contains metadata about the role:
  âš¡ dependencies
  âš¡ author
  âš¡ supported platforms

âœ”ï¸ tests/
Optional directory for unit tests.
Ensures tasks behave as expected.

âœ”ï¸ README.md
Documentation for the role.
Helps in sharing roles with teams.

ğŸ”„ How Playbooks Look When Using Roles
Instead of writing 200 lines of tasks inside playbooks, we simplify:

- name: Configure Kubernetes cluster
  hosts: all

  roles:
    - kubernetes

ğŸ‘‰ The playbook becomes clean and readable.
ğŸ‘‰ Ansible automatically loads tasks from tasks/main.yml.

ğŸ”¹ How This Helps in the Real-World Example
â–¶ï¸ Terraform
Creates 3 EC2 instances.

â–¶ï¸ Ansible with Roles
â¤ Role: Kubernetes
  Inside tasks you can split logic:
  âœ”ï¸ master.yml
  âœ”ï¸ worker.yml
  âœ”ï¸ common.yml
â¤ Clean separation of responsibilities.

ğŸ”„ Benefits:
âœ”ï¸ Modular
âœ”ï¸ Reusable
âœ”ï¸ Scalable
âœ”ï¸ Team-friendly
âœ”ï¸ Easy debugging
Best-practice approach followed in real DevOps workflows


ğŸ”· Infrastructure as Code (IaC)
Infrastructure as Code (IaC) is the practice of managing and provisioning infrastructure (servers, networks, databases, load balancers, etc.) using code instead of manual processes.

The entire infrastructure is written, version-controlled, tested, and reproduced just like software code.

ğŸ”„ Why IaC? 
Traditional infrastructure provisioning is:
  âœ”ï¸ Manual
  âœ”ï¸ Error-prone
  âœ”ï¸ Slow
  âœ”ï¸ Inconsistent

IaC solves these problems by:
  âœ”ï¸ Automating infrastructure creation
  âœ”ï¸ Ensuring consistency across environments
  âœ”ï¸ Making environments reproducible & scalable

ğŸ”„ Key Benefits
âœ”ï¸ Speed & Automation
  â¤ Infrastructure can be deployed in seconds or minutes.
  â¤ No manual clicking in cloud consoles.

âœ”ï¸ Consistency & Repeatability
  â¤ Same configuration across dev â†’ test â†’ prod.
  â¤ Eliminates "it works on my machine" problems.

âœ”ï¸ Version Control
  â¤ IaC files stored in Git.
  â¤ Easy rollback, change tracking, and collaboration.

âœ”ï¸ Scalability
  â¤ Infrastructure can be scaled with code modifications.
  â¤ Supports auto-scaling, replication, etc.

âœ”ï¸ Cost Efficiency
  â¤ Automatically destroy unused resources.
  â¤ Avoids idle infrastructure.

âœ”ï¸ Documentation
  â¤ Code itself becomes documentation.
  â¤ Easy onboarding for teams.

ğŸ”„ Two Approaches to IaC
1ï¸âƒ£ Declarative (What to create)
You specify desired state; tool figures out how to achieve it.
Tools:
  âœ”ï¸ Terraform
  âœ”ï¸ CloudFormation
  âœ”ï¸ Ansible (partially)

2ï¸âƒ£ Imperative (How to create)
You write step-by-step instructions.
Tools:
  âœ”ï¸ Ansible
  âœ”ï¸ Chef / Puppet

ğŸ”„ Popular IaC Tools & Use Cases
âœ”ï¸ Terraform (Most Popular)
  â¤ Cloud-agnostic
  â¤ Manages multi-cloud infrastructure
  â¤ Declarative
  â¤ Best for provisioning servers, networks, databases, load balancers

âœ”ï¸ Ansible
  â¤ Configuration management
  â¤ Also supports provisioning via modules
  â¤ Agentless and uses SSH
  â¤ Best for installing software, managing apps, configurations

ğŸ”„ Real-World Workflow Example
Goal: Deploy a Kubernetes cluster

Step 1ï¸âƒ£ â†’ Provision servers
âœ”ï¸ Terraform creates 3 EC2 instances
âœ”ï¸ Terraform outputs instance IPs

Step 2ï¸âƒ£ â†’ Configure servers
âœ”ï¸ Ansible configures
  â¤ 1 master node
  â¤ 2 worker nodes

Step 3ï¸âƒ£ â†’ Continuous Delivery
âœ”ï¸ GitHub Actions triggers Terraform + Ansible
âœ”ï¸ Environments reproducible anytime

This is a typical DevOps pipeline using IaC.


ğŸ”· Terraform - API as code 
Terraform is not just an Infrastructure-as-Code (IaC) tool.
It is fundamentally an API orchestration engine, meaning:

Terraform converts simple HCL configuration into a series of structured API calls to cloud providers (AWS, Azure, GCP, Kubernetes, GitHub, etc.).

This pattern is called:
â­ â€œAPI as Codeâ€
Because you write declarative code, and Terraform translates it into hundreds of optimized API calls to create, update, or delete cloud resources.

â“ WHY Terraform = â€œAPI as Codeâ€?
  âœ”ï¸ Every cloud provider exposes an HTTP-based API. For example:
  âœ”ï¸ AWS uses AWS REST APIs / JSON
  âœ”ï¸ Azure uses ARM APIs
  âœ”ï¸ GCP uses Cloud REST APIs
  âœ”ï¸ Kubernetes uses K8s API server
  âœ”ï¸ GitHub uses GitHub REST APIs

Terraform simply turns this:
resource "aws_s3_bucket" "demo" {
  bucket = "my-bucket-123"
  acl    = "private"
}

Into highly detailed API calls, like:
PUT https://s3.amazonaws.com/my-bucket-123
{
  "ACL": "private"
}
But you never have to write API calls manually.

Terraform handles:
âœ” Authentication
âœ” Pagination
âœ” Retry logic
âœ” Error handling
âœ” API throttling
âœ” Dependency ordering
âœ” Resource state synchronization

Thus:
Terraform is an abstraction layer on top of APIs â†’ API as Code.

ğŸ”„ How Terraform Actually Works (Internal Workflow)
1ï¸âƒ£ You Write Desired State in HCL

âœ… Example:
resource "aws_instance" "server" {
  ami           = "ami-123456"
  instance_type = "t2.micro"
}


You define what you want, not how to get it.

2ï¸âƒ£ Terraform Builds a Dependency Graph
Terraform automatically determines creation/update order:

VPC â†’ Subnet â†’ EC2 â†’ Security Group â†’ Load Balancer

This is based on:
  â¤ resource references (aws_instance.server)
  â¤ implicit relationships
  â¤ explicit depends_on

3ï¸âƒ£ Terraform Reads the Current State (state file)
State contains:
  âœ”ï¸ resource IDs
  âœ”ï¸ API metadata
  âœ”ï¸ configuration
  âœ”ï¸ dependencies
Terraform uses this to understand differences between desired and current state.

4ï¸âƒ£ Terraform Executes API Calls in Correct Order
Terraform providers convert your code into actual cloud API calls:
| Terraform Resource | API Called         |
| ------------------ | ------------------ |
| aws_instance       | EC2 API            |
| aws_s3_bucket      | S3 API             |
| kubernetes_deploy  | K8s API            |
| github_repo        | GitHub API         |
| cloudflare_record  | Cloudflare DNS API |


Terraform handles:
  âœ”ï¸ rate limiting
  âœ”ï¸ retries
  âœ”ï¸ batching
  âœ”ï¸ request signing
  âœ”ï¸ authentication tokens
  âœ”ï¸ partial failures

5ï¸âƒ£ Terraform Updates State File After API Success
State file = â€œtruth of what existsâ€
Used for:
  âœ”ï¸ drift detection
  âœ”ï¸ future updates
  âœ”ï¸ deleting orphan resources
  âœ”ï¸ parallel execution

ğŸ”„ Terraform Orchestrates APIs, Not Infrastructure
âœ… Examples where Terraform manages APIs without provisioning infra:

âœ” GitHub
resource "github_repository" "repo" {
  name = "devops-project"
}
No servers. Just API operations.

âœ” Cloudflare DNS
resource "cloudflare_record" "dns" {
  name  = "domain.com"
  value = "8.8.8.8"
}

âœ” PagerDuty, Datadog, Slack, Zoom, Okta, Fastly, Stripe
Terraform manages SaaS APIs everywhere.
Thus, Terraform = Universal API Management Framework.

ğŸ”„ Advantages of Terraform as API as Code
1ï¸âƒ£ One Tool for All Cloud APIs
AWS + Azure + GCP + GitHub + Kubernetes + Cloudflare in one codebase.

2ï¸âƒ£ Declarative Code Instead of Imperative Scripts
You declare desired state, Terraform figures out API calls.

3ï¸âƒ£ Automatic Dependency Management
Terraform builds a DAG (directed acyclic graph).

4ï¸âƒ£ Idempotency
Run terraform apply 100 times â†’ same result.

5ï¸âƒ£ Drift Detection
Terraform compares state vs real infra and shows differences.

6ï¸âƒ£ Cost & Time Savings
Avoid writing complex scripts to manage APIs manually.


ğŸ§­ Where Terraform Excels vs Cloud SDKs
| Task               | Terraform   | SDKs or CLI          |
| ------------------ | ----------- | -------------------- |
| Create resources   | Declarative | Manual scripting     |
| Update safely      | Auto        | Hard (diff manually) |
| Handle API retries | Auto        | Must code            |
| State mgmt         | Yes         | No                   |
| Drift detection    | Yes         | No                   |
| Multi-cloud        | Yes         | No                   |


Terraform is more abstract and safer than directly calling cloud APIs.

ğŸ”„ Best Practices for Terraform API-as-Code
â¤ Use modules to avoid repeated configurations.
â¤ Use remote state (S3, Terraform Cloud) for teams.
â¤ Lock state using DynamoDB (AWS).
â¤ Enable versioning for state.
â¤ Use CI/CD to automate terraform plan and apply.
â¤ Follow least privilege IAM for Terraform.

ğŸ’¡ Real-World Examples of API as Code
âœ” Infrastructure provisioning
â¤ EC2, VPC, RDS, S3, containers, servers

âœ” SaaS configuration
â¤ GitHub repos, Cloudflare DNS, Datadog alerts, Slack channels

âœ” Kubernetes workflows
â¤ Deployments, services, ingress, ConfigMaps

âœ” CI/CD pipelines
â¤ Using GitHub, GitLab, CircleCI providers

âœ” User & Identity Management
â¤ Using Okta, Azure AD providers


ğŸ”· How to Install Terraform (Step-by-Step)
âœ… Install Terraform on EC2 (Amazon Linux 2 / Amazon Linux)

Terraform is not available through yum by default â€” but HashiCorp provides a yum repository.

â­ Method 1ï¸âƒ£ (Recommended â€” Official HashiCorp Repo)
Works for Amazon Linux 2 and Amazon Linux 2023.

1ï¸âƒ£ Add HashiCorp Yum Repo
Run the command:

ğŸ”— sudo yum install -y yum-utils
ğŸ”— sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo

2ï¸âƒ£ Install Terraform
ğŸ”— sudo yum -y install terraform

3ï¸âƒ£ Verify Installation
ğŸ”— terraform -v

You should see something like:
Terraform v1.9.x

ğŸ‰ Done! Terraform is installed on your EC2 instance.

â­ Method 2 (Manual Binary Install â€” Works on Any Linux)
Use this if repo installation fails.

1ï¸âƒ£ Download Terraform Binary
ğŸ”— sudo yum install -y wget unzip
ğŸ”— wget https://releases.hashicorp.com/terraform/1.9.6/terraform_1.9.6_linux_amd64.zip


ğŸ”— (check latest version: https://developer.hashicorp.com/terraform/downloads
)

2ï¸âƒ£ Unzip and Move to /usr/local/bin
ğŸ”— unzip terraform_1.9.6_linux_amd64.zip
ğŸ”— sudo mv terraform /usr/local/bin/

3ï¸âƒ£ Verify
ğŸ”— terraform -v

â­ Method 3 (Ubuntu EC2 Instance)
ğŸ”— sudo apt-get update && sudo apt-get install -y gnupg software-properties-common

ğŸ”— wget -O- https://apt.releases.hashicorp.com/gpg | \
  sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg

ğŸ”— echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \
ğŸ”— https://apt.releases.hashicorp.com $(lsb_release -cs) main" | \
ğŸ”— sudo tee /etc/apt/sources.list.d/hashicorp.list

ğŸ”— sudo apt update
ğŸ”— sudo apt install terraform -y

ğŸŸ¢ Check installation
ğŸ”— terraform version

âœ¨ Terraform is now fully ready on your EC2 instance.

âœ… 4. Terraform Project Folder Structure
terraform-project/
â”‚â”€â”€ main.tf
â”‚â”€â”€ variables.tf
â”‚â”€â”€ outputs.tf
â”‚â”€â”€ provider.tf
â”‚â”€â”€ terraform.tfvars

â¤ provider.tf â†’ Specifies AWS/Azure/GCP provider
â¤ main.tf â†’ Contains main resources
â¤ variables.tf â†’ Input variables
â¤ terraform.tfvars â†’ Values for variables
â¤ outputs.tf â†’ Output values after provisioning

ğŸ”„ Create Your First Terraform Project
Below is a simple example to create an AWS EC2 instance.

1ï¸âƒ£ provider.tf
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.region
}

2ï¸âƒ£ variables.tf
variable "region" {
  type        = string
  description = "AWS region to deploy resources"
}

variable "instance_type" {
  type        = string
  default     = "t2.micro"
}

variable "ami" {
  type        = string
  description = "AMI ID for EC2 instance"
}

3ï¸âƒ£terraform.tfvars
region = "ap-south-1"  # Mumbai
ami    = "ami-0c2b8ca1dad447f8a"

4ï¸âƒ£ main.tf
resource "aws_instance" "web_server" {
  ami           = var.ami
  instance_type = var.instance_type

  tags = {
    Name = "MyTerraformServer"
  }
}

5ï¸âƒ£ outputs.tf
output "instance_id" {
  value = aws_instance.web_server.id
}

output "public_ip" {
  value = aws_instance.web_server.public_ip
}

Terraform Commands (Full Workflow)
1ï¸âƒ£ Initialize project
ğŸ”— Downloads providers.
ğŸ”— terraform init

2ï¸âƒ£ Format code
ğŸ”— terraform fmt

3ï¸âƒ£ Validate syntax
ğŸ”— terraform validate

4ï¸âƒ£ Preview changes
ğŸ”— terraform plan

5ï¸âƒ£ Apply changes
ğŸ”— terraform apply
Type yes to confirm.

6ï¸âƒ£ Destroy everything
ğŸ”— terraform destroy


ğŸ”„ How Terraform Works Internally 
Terraform has 3 phases:

1ï¸âƒ£ Terraform Prepare (Init)
Downloads providers and configures backend.

2ï¸âƒ£ Terraform Plan

Compares:
Desired state (in code)
â†“
Real infra state (in cloud)
Outputs an execution plan.

3ï¸âƒ£ Terraform Apply
Applies the changes and updates the state file (terraform.tfstate).

ğŸ”„ Terraform State File 
Terraform creates:
ğŸ”— terraform.tfstate
This file stores the current state of your infrastructure.

State File Must Be:
  âœ”ï¸ Not manually edited
  âœ”ï¸ Stored in a remote backend like S3 in teams
  âœ”ï¸ Version-controlled (but not uploaded to GitHub!)

ğŸ”„ Best Practices for Terraform
âœ” Use separate files: main.tf, variables.tf, outputs.tf
âœ” Always run plan before apply
âœ” Never commit terraform.tfstate
âœ” Use remote backend (S3, GCS) for teams
âœ” Keep modules for reusable code
âœ” Use variable validation

Configure your AWS provider
ğŸ”— aws configure

âœ… EC2 Instance creation Example: main.tf
provider "aws" {
  region = "us-west-2"
}

data "aws_ami" "ubuntu" {
  most_recent = true

  filter {
    name = "name"
    values = ["ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*"]
  }

  owners = ["099720109477"] # Canonical
}

resource "aws_instance" "app_server" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t2.micro"

  tags = {
    Name = "learn-terraform"
  }
}

1ï¸âƒ£ Provider Block
provider "aws" {
  region = "us-west-2"
}
â¤ Tells Terraform to use the AWS provider.
â¤ The region for all AWS resources will be us-west-2 (Oregon).
â¤ Terraform will look for AWS credentials locally (via ~/.aws/credentials, environment variables, or aws configure).

2ï¸âƒ£ Data Source Block â€” Fetch Latest Ubuntu AMI
data "aws_ami" "ubuntu" {
  most_recent = true

  filter {
    name = "name"
    values = ["ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*"]
  }

  owners = ["099720109477"] # Canonical
}

â“ What is a data source?
A data source lets Terraform read information from AWS (but not create it).
It fetches data that already exists in AWS.

â“ What this specific data source does:
It queries AWS to find:
  âœ”ï¸ the latest Ubuntu 24.04 ("Noble") AMI
  âœ”ï¸ the official AMI published by Canonical (owner ID: 099720109477)
  âœ”ï¸ AMIs matching the pattern:
    ğŸ”— "ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*"

â“ Why use most_recent = true?
Because Ubuntu published many AMIs over time.

This ensures:
â¡ Always pick the latest patch release
â¡ No need to manually update AMI IDs

â“ Why use owner = Canonical?
Prevents using fake or insecure AMIs.

â“ How Terraform Processes This Block
â¤ Terraform sends an API call to describe images:
aws ec2 describe-images --owners 099720109477 --filters ...

ğŸ‘‰ Then picks the most recent result.

3ï¸âƒ£ EC2 Resource Block
resource "aws_instance" "app_server" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t2.micro"

  tags = {
    Name = "learn-terraform"
  }
}
ğŸ‘‰ This block creates a new EC2 instance.

1ï¸âƒ£ AMI
ami = data.aws_ami.ubuntu.id

â¤ Uses the ID fetched from the data source.
â¤ Terraform ensures the data source executes before creating the EC2 instance.

2ï¸âƒ£ Instance Type
instance_type = "t2.micro"

â¤ Free-tier eligible small instance

3ï¸âƒ£ Tags
tags = {
  Name = "learn-terraform"
}
â¤ Helps identify the instance in the AWS console.

ğŸ”„ Terraform Execution Flow
When you run:
  â¤ terraform init
  â¤ terraform plan
  â¤ terraform apply

This happens: 
1ï¸âƒ£ init
Downloads AWS provider plugin.

2ï¸âƒ£ plan
â¤ Queries AWS â†’ retrieves latest Ubuntu AMI ID.
â¤ Shows what EC2 instance will be created.
â¤ No resources created yet.

3ï¸âƒ£ apply
â¤ Creates the EC2 instance using:
â¤ region: us-west-2
â¤ AMI: (latest Ubuntu 24.04)
â¤ type: t2.micro
â¤ tags: learn-terraform


ğŸ”· Terraform Remote Backend on AWS (S3 + DynamoDB Locking)
This is the industry-standard setup used in production for teams.

You get:
  âœ” Centralized shared state for all team members
  âœ” Automatic state versioning
  âœ” State locking (prevents multiple people applying at the same time)
  âœ” Safe, secure storage

ğŸŸ© Architecture Overview
Terraform â†’ S3 bucket (stores state)
             â†“
          DynamoDB table (locks state)

1ï¸âƒ£ Create an S3 Bucket for Storing Terraform State
You can do this manually or using AWS CLI:

AWS CLI (Recommended)
aws s3api create-bucket --bucket your-terraform-state-bucket --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2

Turn on versioning (very important):
aws s3api put-bucket-versioning --bucket your-terraform-state-bucket --versioning-configuration Status=Enabled

2ï¸âƒ£ Create DynamoDB Table for State Locking
aws dynamodb create-table \
  --table-name terraform-locks \
  --attribute-definitions AttributeName=LockID,AttributeType=S \
  --key-schema AttributeName=LockID,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST
ğŸ‘‰ This table prevents multiple people from running terraform apply at the same time.

3ï¸âƒ£ Add Backend Block in Terraform
In your Terraform project, create or update backend.tf:

terraform {
  backend "s3" {
    bucket         = "your-terraform-state-bucket"
    key            = "ec2-project/terraform.tfstate"
    region         = "us-west-2"
    dynamodb_table = "terraform-locks"
    encrypt        = true
  }
}

Meaning of fields:
| Field            | Explanation                                         |
| ---------------- | --------------------------------------------------- |
| `bucket`         | S3 bucket holding the state                         |
| `key`            | Path/file inside S3 bucket (you can group projects) |
| `region`         | Must match the S3 bucket region                     |
| `dynamodb_table` | Enables state locking                               |
| `encrypt=true`   | Enables SSE encryption on S3                        |


4ï¸âƒ£ Run Terraform Init (IMPORTANT)
terraform init

You will see something like:
Initializing the backend...
Terraform detected a remote backend. Do you want to migrate? (yes)

Type yes.
This will:
Upload your local terraform.tfstate to S3

Enable locking with DynamoDB

5ï¸âƒ£ Verify
Check S3:

You should see:
ec2-project/
  terraform.tfstate
  terraform.tfstate.backup

Check DynamoDB:
A lock entry will appear during terraform apply.

â­ Why Use S3 + DynamoDB Backend?
âœ” Team Collaboration
State is stored centrally, not on developer laptops.

âœ” Prevent Corruption
DynamoDB locking ensures only 1 person can run apply at a time.

âœ” Versioning
Roll back to previous states easily using S3 versioning.

âœ” Secure
S3 encryption
IAM policies
Access control
Locked state

ğŸŸ© IAM Permissions Required
Your AWS user/role needs permission for:

S3
s3:PutObject
s3:GetObject
s3:ListBucket
s3:DeleteObject

DynamoDB
dynamodb:PutItem
dynamodb:GetItem
dynamodb:DeleteItem
dynamodb:UpdateItem


ğŸ”· How to Write Modules in Terraform
Terraform modules help you organize, reuse, and standardize your infrastructure code.
A module is simply a folder containing Terraform files (.tf).

ğŸŒŸ  Basic Structure of a Terraform Module
modules/
  â””â”€â”€ ec2/
       â”œâ”€â”€ main.tf
       â”œâ”€â”€ variables.tf
       â”œâ”€â”€ outputs.tf

Your root project:
project/
  â”œâ”€â”€ main.tf
  â”œâ”€â”€ providers.tf
  â”œâ”€â”€ variables.tf
  â””â”€â”€ modules/
        â””â”€â”€ ec2/


ğŸŒŸ  Creating a Module
1ï¸âƒ£ Create a folder for module
âœ… Example: modules/ec2

2ï¸âƒ£ Write the Module Code
ğŸ”„ main.tf
This contains the actual resource definition.
resource "aws_instance" "this" {
  ami           = var.ami
  instance_type = var.instance_type

  tags = {
    Name = var.name
  }
}


ğŸ”„ variables.tf
Define inputs your module will receive:
variable "ami" {
  type        = string
  description = "AMI ID for the EC2 instance"
}

variable "instance_type" {
  type        = string
  description = "Instance type"
}

variable "name" {
  type        = string
  description = "Name tag"
}


ğŸ”„ outputs.tf
Outputs that callers of the module can use:
output "instance_id" {
  value = aws_instance.this.id
}

output "public_ip" {
  value = aws_instance.this.public_ip
}


3ï¸âƒ£ Using the Module in Your Root Code
Your projectâ€™s main.tf:
module "my_server" {
  source        = "./modules/ec2"
  ami           = "ami-0abcd1234"
  instance_type = "t2.micro"
  name          = "my-ec2-instance"
}

4ï¸âƒ£ Initialize and Apply
terraform init
terraform plan
terraform apply


ğŸ”„ Key Best Practices (Interview-Level Points)
â¤ A module must have:
  âœ” main.tf
  âœ” variables.tf
  âœ” outputs.tf
â¤ Keep modules reusable and stateless
â¤ Avoid hard-coded values
â¤ Publish reusable modules to:
  âœ”ï¸ GitHub
  âœ”ï¸ Terraform Registry

Use versioning while using remote modules:
source  = "git::https://github.com/user/repo.git//modules/ec2?ref=v1.0.0"


âœ… Example: Real-World Use Case
Want to create 3 different EC2 servers?
You reuse the module:
module "web" {
  source = "./modules/ec2"
  ami = "ami-123"
  instance_type = "t2.micro"
  name = "web-server"
}

module "db" {
  source = "./modules/ec2"
  ami = "ami-123"
  instance_type = "t3.medium"
  name = "db-server"
}

module "cache" {
  source = "./modules/ec2"
  ami = "ami-123"
  instance_type = "t3.small"
  name = "cache-server"
}


ğŸš¨ Common Problems With Terraform
1ï¸âƒ£ State File Issues (Most Common Problem)
Terraform stores infrastructure state in terraform.tfstate.

ğŸ”„ Problems:
â¤ State file corruption if two people run Terraform at the same time.
â¤ Sensitive data exposure (passwords, secrets appear in plain JSON).
â¤ State drift â†’ Infra changed manually in console, state becomes outdated.
â¤ Locking issues:
  âœ”ï¸ If DynamoDB lock not configured, parallel runs break the state.
 
â— Why this is serious:
State is the single source of truth; if it breaks, infra breaks.

2ï¸âƒ£ Terraform Is Declarative, So Debugging Is Hard
You cannot step-through like code.

â— Problems:
â¤ Hard to understand why a resource is being recreated.
â¤ Terraform plan sometimes acts unexpectedly:
â¤ Changing a tag causes resource replacement.
â¤ Small changes create large diffs.

3ï¸âƒ£terraform â€œDependency Hellâ€ Inside Modules
Modules are reusable, butâ€¦

â—Problems:
â¤ Wrong variable types break modules.
â¤ Implicit dependencies can cause unpredictable order of creation.
â¤ Large module chains become hard to manage.

4ï¸âƒ£ Provider Bugs & Version Issues
Terraform relies on external providers (AWS, GCP, K8s, etc.)

â—Common issues:
â¤ Providers get updated frequently â†’ breaks your code.
â¤ AWS releases new features slowly in Terraform provider.
â¤ Providers sometimes have breaking changes between versions.

âœ… Example:
â¤ Error: Provider registry.terraform.io hash mismatch

5ï¸âƒ£ Terraform Destroy Is Dangerous
terraform destroy deletes everything in the workspace.

â—Problems:
â¤ Accidental destroy (wrong workspace)
â¤ Running destroy in production by mistake
â¤ Soft deletion doesnâ€™t exist; everything is permanently deleted

6ï¸âƒ£ Complex Real-World Infra Becomes Hard to Manage
Terraform struggles with:

â—Problems:
â¤ 1000+ resources = huge plan/apply time.
â¤ Understanding â€œwhy Terraform wants to recreate resource X.â€
â¤ Interdependent resources across microservices.

7ï¸âƒ£ Lack of Built-in Secrets Management
â—Terraform does NOT:
  â¤ Encrypt variables automatically
  â¤ Manage secret rotation
  â¤ Mask secrets in logs
Secrets appear in:
  â¤ tfstate
  â¤ plan output
  â¤ console logs

8ï¸âƒ£ Poor Handling of Conditional Logic
Terraform uses HCL which is limited:

â—Problems:
â¤ No loops like programming languages
â¤ No complex if-else logic
â¤ Cannot dynamically create nested structures easily

Workarounds become messy:
ğŸ”— count = var.enabled ? 1 : 0

9ï¸âƒ£ Difficult to Test
Terraform has no native testing framework.

â—Problems:
â¤ No easy â€œunit testsâ€
â¤ Terratest/go tests are slow and expensive
â¤ Hard to simulate cloud infra

1ï¸âƒ£0ï¸âƒ£ Slow Performance in Large Projects
â—Reasons:
â¤ Large number of resources
â¤ Provider refresh takes long
â¤ Dependency graph resolution
In big orgs, a single terraform apply can take 20-40 minutes.

1ï¸âƒ£1ï¸âƒ£ Limited Error Messages
Terraform errors are often unclear:

Error: Invalid index
Error: attribute not found
Error: Unknown provider
âœ… Example: One wrong comma can break 100+ resources.

1ï¸âƒ£2ï¸âƒ£ Workspaces Are Misleading
Many think workspaces = multiple environments.
Wrong.

â—Problems:
â¤ Workspaces share the same code but not designed for prod/staging/dev separation.
â¤ Easy to apply prod config into dev or vice versa.
â¤ Causes accidental resource deletion.

1ï¸âƒ£3ï¸âƒ£ Drift Detection Is Weak
Terraform only sees drift when you run:
terraform plan

No automatic alerts
No visualization
No continuous drift monitoring (unless using Terraform Cloud)

1ï¸âƒ£4ï¸âƒ£ Circular Dependencies
Terraform cannot resolve circular resource dependencies.

âœ… Example:
VPC requires subnet
Subnet requires VPC output
â†’ Terraform cannot compute graph

1ï¸âƒ£5ï¸âƒ£ Destroy/Apply Ordering Issues
Sometimes resource deletion fails due to dependency conflicts:

Error: DependencyViolation: Cannot delete VPC because subnet still exists
Even though Terraform â€œthinksâ€ order is correct.


ğŸ”· CI/CD (Continuous Integration/ Continuous Delivery)
CI/CD stands for Continuous Integration and Continuous Deployment (or Continuous Delivery).
It is a set of DevOps practices that automate the process of building, testing, and deploying software.

ğŸ”„ CI â€” Continuous Integration
â¤ Continuous Integration means:
â¤ Developers frequently push code to a shared repo (GitHub/GitLab/Bitbucket)
â¤ Every push triggers:
  âœ”ï¸ Automatic build
  âœ”ï¸ Automatic tests
  âœ”ï¸ Code quality checks

ğŸ‘‰ Goal:
Catch bugs early, integrate code smoothly, avoid â€œworks on my machineâ€ issues.


ğŸ”„ CD â€” Continuous Deployment / Continuous Delivery
There are two meanings of CD:

1ï¸âƒ£ Continuous Delivery
â¤ Code is automatically built and tested.
â¤ Deployment to production requires manual approval.

ğŸ‘‰ Goal:
Production-ready code at all times.

2ï¸âƒ£ Continuous Deployment
â¤ Fully automated.
â¤ After tests pass, code goes straight to production with no manual approval.

ğŸ‘‰ Goal:
Faster releases.

âœ… Example Flow:
1ï¸âƒ£ Developer pushes code to GitHub
2ï¸âƒ£ CI server (GitHub Actions, Jenkins, GitLab CI, CircleCI) runs:
  âœ”ï¸ Build
  âœ”ï¸ Tests
  âœ”ï¸ Code quality scan

3ï¸âƒ£ If everything passes â†’ CD triggers
4ï¸âƒ£ App is deployed automatically to:
  âœ”ï¸ AWS
  âœ”ï¸ Docker
  âœ”ï¸ Kubernetes
  âœ”ï¸ Azure
  âœ”ï¸ GCP

â­ Tools Used:
CI Tools
  âœ”ï¸ GitHub Actions
  âœ”ï¸ GitLab CI
  âœ”ï¸ Jenkins
  âœ”ï¸ CircleCI
  âœ”ï¸ TravisCI

CD Tools
  âœ”ï¸ ArgoCD
  âœ”ï¸ Spinnaker
  âœ”ï¸ Jenkins
  âœ”ï¸ GitHub Actions
  âœ”ï¸ AWS CodeDeploy

ğŸ”· Jenkins
Jenkins is an open-source automation server used to build, test, and deploy applications.
It is one of the oldest and most popular CI/CD tools.

ğŸ”„ Key points :
â¤ Written in Java
â¤ Self-hosted (you install and manage it)
â¤ Highly extensible with 1800+ plugins
â¤ Used to automate CI/CD pipelines
â¤ Works with almost every technology

ğŸ”„ How Jenkins Works (Simple Explanation)
Jenkins follows a masterâ€“agent architecture (also called controllerâ€“worker).

1ï¸âƒ£ Jenkins Controller (Master)
â¤ Stores configurations
â¤ Schedules jobs
â¤ Distributes tasks to agents
â¤ Handles UI, plugins, secrets, etc.

2ï¸âƒ£  Jenkins Agent (Worker)
â¤ Executes jobs (build, test, deploy)
â¤ Can run on:
  âœ”ï¸ Linux
  âœ”ï¸ Windows
  âœ”ï¸ Docker
  âœ”ï¸ Kubernetes

ğŸš€ Jenkins Workflow Step-by-Step
1ï¸âƒ£ Developer pushes code to GitHub/GitLab
2ï¸âƒ£ Git webhook triggers Jenkins
3ï¸âƒ£ Jenkins fetches the latest code
4ï¸âƒ£ Jenkins pipeline executes:
  âœ”ï¸ Build
  âœ”ï¸ Test
  âœ”ï¸ Code scan
  âœ”ï¸ Artifact creation
  âœ”ï¸ Deployment
5ï¸âƒ£ Jenkins reports success/failure
6ï¸âƒ£ Notifications sent (Slack, email, etc.)

âœ… Example Jenkinsfile
pipeline {
    agent any
    stages {
        stage('Build') {
            steps { sh 'mvn clean install' }
        }
        stage('Test') {
            steps { sh 'mvn test' }
        }
        stage('Deploy') {
            steps { sh './deploy.sh' }
        }
    }
}

ğŸ”¥ Jenkins vs GitHub Actions â€” Similarities & Differences
âš¡ SIMILARITIES
| Feature                            | Jenkins     | GitHub Actions         |
| ---------------------------------- | ----------- | ---------------------- |
| CI/CD automation                   | âœ” Yes       | âœ” Yes                  |
| Pipeline as code                   | Jenkinsfile | workflow YAML          |
| Supports GitHub, GitLab, Bitbucket | âœ” Yes       | âœ” Mainly GitHub (best) |
| Supports build, test, deploy       | âœ”           | âœ”                      |
| Integrations with cloud services   | âœ”           | âœ”                      |
| Docker/Kubernetes support          | âœ”           | âœ”                      |


In short:
Both are CI/CD tools that automate software development workflows.

ğŸ”· GitHub Actions
GitHub Actions is a cloud-based CI/CD platform built directly into GitHub that allows you to automate workflows such as building, testing, and deploying your code whenever changes occur in your repository.

It lets you run automation on GitHubâ€™s servers without installing anything.


ğŸ”„ Why it is popular
â¤ 100% integrated with GitHub
â¤ Zero setup, no server to manage
â¤ Easy YAML-based pipeline
â¤ Free minutes for public repos
â¤ Huge marketplace of pre-built actions
â¤ Supports multi-cloud deployments

ğŸ”„ How GitHub Actions Works (Simple)
1ï¸âƒ£ You create a .github/workflows/*.yml file
2ï¸âƒ£ Define jobs â†’ steps â†’ commands

3ï¸âƒ£ Trigger the workflow based on events like:
â¤ push
â¤ pull_request
â¤ schedule (cron)
â¤ release

4ï¸âƒ£ GitHub runs your pipeline on its hosted VMs (runners)
5ï¸âƒ£ The workflow executes:
â¤ builds
â¤ runs tests
â¤ deploys to cloud/Kubernetes/Docker/etc.

âœ… Example of GitHub Actions Workflow
ğŸ”— .github/workflows/ci.yml:

name: CI Pipeline

on:
  push:
    branches: ["main"]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18

      - name: Install dependencies
        run: npm install

      - name: Run tests
        run: npm test

ğŸ”¥ What can GitHub Actions automate?
âœ” CI (Continuous Integration)
âœ”ï¸ Run tests automatically
âœ”ï¸ Build code automatically
âœ”ï¸ Enforce quality checks

âœ” CD (Continuous Deployment)
Deploy to:
âœ”ï¸ AWS
âœ”ï¸ Azure
âœ”ï¸ GCP
âœ”ï¸ Docker
âœ”ï¸ Kubernetes
âœ”ï¸ Vercel
âœ”ï¸ Netlify

âœ” Other Automation
âœ”ï¸ Auto-label PRs
âœ”ï¸ Auto-assign reviewers
âœ”ï¸ Auto-merge PRs
âœ”ï¸ Run scheduled jobs (cron)
âœ”ï¸ Build & publish Docker images

ğŸ“Œ Key Components
1ï¸âƒ£Workflow
â¤ A YAML file that defines automation.

2ï¸âƒ£ Jobs
â¤ A set of steps executed in a VM.

3ï¸âƒ£ Steps
â¤ Shell commands or reusable actions.

4ï¸âƒ£ Runners
â¤ Machines on which workflows run:
â¤ GitHub-hosted
â¤ Self-hosted

5ï¸âƒ£ Actions
â¤ Reusable automation components from the marketplace.


âš¡ DIFFERENCES BETWEEN JENKINS AND GITHUB ACTIONS
1ï¸âƒ£ Hosting
âš¡ Jenkins â†’ Self-hosted
â¤ You install and maintain Jenkins server
â¤ Cost depends on your infrastructure

âœ”ï¸ GitHub Actions â†’ Cloud-hosted
â¤ No server needed
â¤ Free minutes included
â¤ Scales automatically

2ï¸âƒ£ Setup & Maintenance
âš¡ Jenkins:
â¤ Harder to set up
â¤ Need to manage plugins
â¤ Need to maintain servers, upgrades, backups
â¤ Best for enterprise-level custom setups

âœ”ï¸ GitHub Actions:
â¤ Zero setup
â¤ No maintenance
â¤ Pipeline defined in a few lines of YAML

3ï¸âƒ£ Integration
âš¡ Jenkins:
â¤ Works with ANY repository (GitHub, GitLab, Bitbucket)

âœ”ï¸ GitHub Actions:
â¤ Best experience with GitHub repositories
â¤ Native integration: issues, PR checks, secrets, packages

4ï¸âƒ£ Plugins
âš¡ Jenkins: 1800+ plugins, unlimited customization
âœ”ï¸ GitHub Actions: Marketplace actions (limited compared to Jenkins)

5ï¸âƒ£ Scalability
âš¡ Jenkins:
â¤ Needs configuring agents, nodes, clusters
â¤ More work but highly flexible

âœ”ï¸ GitHub Actions:
â¤ Auto-scales
â¤ No need to manage agents unless using self-hosted runners

6ï¸âƒ£ Learning Curve
âš¡ Jenkins: Steeper (Groovy, plugins, server setup)
âœ”ï¸ GitHub Actions: Easier (simple YAML)

7ï¸âƒ£ Cost
âš¡ Jenkins: Your server cost + maintenance
âœ”ï¸ GitHub Actions: Free minutes + pay-per-use


ğŸ”· How to install Jenkins and configure Docker as slave set up CI/CD deploy application to Kubernetes cluster

1ï¸âƒ£  Install Jenkins (Ubuntu / EC2 / Linux Server)
Step 1ï¸âƒ£ Install Java (Jenkins needs Java 11+)
ğŸ”— sudo apt update
sudo apt install fontconfig openjdk-11-jre -y
ğŸ”— java -version

Step 2ï¸âƒ£ Install Jenkins
ğŸ”— curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo tee \
  /usr/share/keyrings/jenkins-keyring.asc > /dev/null

ğŸ”— echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list > /dev/null

ğŸ”— sudo apt update
ğŸ”— sudo apt install jenkins -y
ğŸ”— sudo systemctl start jenkins
ğŸ”— sudo systemctl enable jenkins

ğŸ”„  Edit the Inbound Rules to allow port 8080
ğŸ”— sudo ufw allow 8080

Step 3ï¸âƒ£ Open Jenkins on Browser
ğŸ”— http://<server-ip>:8080

Step 4ï¸âƒ£ Get Admin Password
ğŸ”— sudo cat /var/lib/jenkins/secrets/initialAdminPassword

Install suggested plugins â†’ create admin user â†’ Jenkins ready.

2ï¸âƒ£  Install Docker on Jenkins Server
sudo apt install docker.io -y
sudo usermod -aG docker jenkins
sudo systemctl restart docker
sudo systemctl restart jenkins

3ï¸âƒ£  Configure Docker Agent (Slave) for Jenkins
â“ Why Docker Agent?
âœ”ï¸ Build isolation
âœ”ï¸ Reproducible CI environments
âœ”ï¸ Easy scaling
âœ”ï¸ Supports container-based builds

ğŸ”„ Option A: Run Docker Agent Using SSH (simple)
On the Docker agent machine:
ğŸ”— sudo apt update
ğŸ”— sudo apt install docker.io -y
ğŸ”— sudo useradd -m jenkins
ğŸ”— sudo usermod -aG docker jenkins
ğŸ”— sudo passwd jenkins

Generate SSH key from Jenkins master:
ğŸ”— ssh-keygen

Copy public key to agent machine:
ğŸ”— ssh-copy-id jenkins@<agent-ip>

In Jenkins:
Manage Jenkins â†’ Manage Nodes â†’ New Node â†’ Permanent Agent

â¤ Remote root directory: /home/jenkins
â¤ Launch method: SSH
â¤ Host: <agent-ip>
â¤ Credentials: SSH key

ğŸ‘‰ Save â†’ agent becomes online.

ğŸ”„ Option B: Connect Jenkins Agent using Docker Container (popular)
Run agent container:
ğŸ”— docker run -d --name jenkins-agent \
  -p 22:22 \
  -e JENKINS_AGENT_SSH_PUBKEY="<your-public-key>" \
  jenkins/ssh-agent

Add this as SSH node in Jenkins â†’ done.

4ï¸âƒ£ Create CI/CD Pipeline to Deploy to Kubernetes
ğŸŒŸ Pipeline Flow
1ï¸âƒ£ Get c ode from GitHub
2ï¸âƒ£ Build Docker image
3ï¸âƒ£ Push image to Docker Hub / ECR
4ï¸âƒ£ Apply Kubernetes manifest
5ï¸âƒ£ Rollout update

5ï¸âƒ£  Install kubectl & give Jenkins access
1ï¸âƒ£ Install kubectl on Jenkins server:
ğŸ”— curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
ğŸ”— chmod +x kubectl
ğŸ”— sudo mv kubectl /usr/local/bin/

ğŸ‘‰ Move kubeconfig to Jenkins

Copy kubeconfig from your cluster:
ğŸ”— sudo mkdir /var/lib/jenkins/.kube
ğŸ”— sudo cp ~/.kube/config /var/lib/jenkins/.kube/config
ğŸ”— sudo chown -R jenkins:jenkins /var/lib/jenkins/.kube

6ï¸âƒ£  Jenkinsfile for CI/CD to Kubernetes
Place this in your repo:
pipeline {
    agent any

    environment {
        REGISTRY = "docker.io/yourname"
        IMAGE = "myapp"
        TAG = "v${BUILD_NUMBER}"
    }

    stages {
        stage('Checkout') {
            steps {
                git 'https://github.com/your/repo.git'
            }
        }

        stage('Build Docker Image') {
            steps {
                sh """
                docker build -t $REGISTRY/$IMAGE:$TAG .
                """
            }
        }

        stage('Push Image') {
            steps {
                withCredentials([string(credentialsId: 'dockerhub-token', variable: 'TOKEN')]) {
                    sh """
                    echo "$TOKEN" | docker login -u yourname --password-stdin
                    docker push $REGISTRY/$IMAGE:$TAG
                    """
                }
            }
        }

        stage('Deploy to Kubernetes') {
            steps {
                sh """
                sed -i "s|IMAGE_TAG|$TAG|g" k8s/deployment.yaml
                kubectl apply -f k8s/deployment.yaml
                """
            }
        }
    }
}

âœ… Example Kubernetes Deployment Manifest
ğŸ”— k8s/deployment.yaml:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: docker.io/yourname/myapp:IMAGE_TAG
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBalancer
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080

CI/CD will replace IMAGE_TAG with actual tag.

ğŸ”„  Final Flow (Complete CICD)
â¤ Developer pushes to GitHub
â¤ Jenkins triggers pipeline
â¤ Docker agent builds image
â¤ Jenkins pushes image to registry
â¤ Jenkins updates Kubernetes manifests
â¤ kubectl apply deploys updated app
â¤ Kubernetes performs rolling update
â¤ Application updated with zero downtime

ğŸŒŠ Jenkins for CI/CD to Kubernetes video number 25


#################
ğŸ”·GitHub Actions
#################
â¤ A CI/CD automation platform built inside GitHub.
â¤ Lets you run automated workflows (pipelines) triggered by events like push, pull request, merge, issue creation, schedule, or manual runs.
â¤ Helps automate tasks such as building, testing, deploying, linting, packaging, etc.


ğŸ”„ Use of Github Actions 
âœ”ï¸ Fast & integrated CI/CD â†’ No external tools needed.
âœ”ï¸ Event-driven automation â†’ Every commit can trigger build/test pipelines.
âœ”ï¸ Cross-platform â†’ Supports Linux, Windows, macOS runners.
âœ”ï¸ Reusable workflows â†’ Saves time & keeps consistency.
âœ”ï¸ Supports containers â†’ Docker, Kubernetes deployments.
âœ”ï¸ Free minutes for public repositories â†’ great for open source.

ğŸ”„ Core Concepts 
1ï¸âƒ£ Workflow
â¤ The entire pipeline defined in a .yml file.
â¤ Located in .github/workflows/.

2ï¸âƒ£ Jobs
â¤ A workflow contains multiple jobs (e.g., build, test, deploy).
â¤ Jobs run parallel unless dependencies are provided.

3ï¸âƒ£ Steps
â¤ Instructions executed inside a job.
â¤ Steps run sequentially.

4ï¸âƒ£ Actions
â¤ Reusable extensions written by anyone (official or community).
âœ… Example: actions/checkout, actions/setup-node.

5ï¸âƒ£ Runners
â¤ Machines that execute workflows.
â¤ Types:
  âœ”ï¸ GitHub-hosted runners (default Linux/macOS/Windows machines)
  âœ”ï¸ Self-hosted runners (your own server/VM)


âœ… Workflow File Example 
ğŸ”— CI pipeline for Node.js

name: CI Pipeline

on:
  push:
    branches: [ main ]
  pull_request:

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node
        uses: actions/setup-node@v3
        with:
          node-version: 18

      - name: Install Dependencies
        run: npm install

      - name: Run Tests
        run: npm test


ğŸ”„  Common Triggers
| Trigger             | Meaning                    |
| ------------------- | -------------------------- |
| `push`              | On committing code         |
| `pull_request`      | On PR creation/update      |
| `workflow_dispatch` | Manual trigger             |
| `schedule`          | Cron jobs                  |
| `release`           | On GitHub release creation |


âœ… Example of scheduled job:
on:
  schedule:
    - cron: "0 0 * * *"   # runs daily at midnight


ğŸ”„ Real-World Use Cases (mention in interview)
1ï¸âƒ£ Automatic Testing
Run unit tests on every PR to prevent breaking code.

2ï¸âƒ£ Deployment Pipelines
â¤ Deploy backend or frontend automatically to:
  âœ”ï¸ AWS EC2
  âœ”ï¸ AWS S3 (static website)
  âœ”ï¸ Docker Hub
  âœ”ï¸ Kubernetes cluster
  âœ”ï¸ Vercel / Netlify / Firebase

3ï¸âƒ£ Build & Publish Artifacts
  âœ”ï¸ Build JAR/WAR files for Java.
  âœ”ï¸ Create Docker images.
  âœ”ï¸ Publish npm or Maven packages.

4ï¸âƒ£ Linting & Code Quality
Auto-run ESLint, Prettier, SonarCloud.

5ï¸âƒ£ CI for Microservices
Each microservice gets its own workflow.


â­ 1. GitHub Actions vs Jenkins
| GitHub Actions     | Jenkins                   |
| ------------------ | ------------------------- |
| Cloud-native       | Self-hosted               |
| No server required | Needs setup & maintenance |
| Fast to start      | More complex              |
| YML-based          | Custom DSL                |


If asked which is better:
â†’ GitHub Actions for simple, quick CI/CD. Jenkins for large enterprises needing fine-grained customization.

â­ 2. Environment Variables & Secrets
â¤ Set secure values under Repository â†’ Settings â†’ Secrets â†’ Actions.
âœ… Example:

env:
  AWS_REGION: ap-south-1

- name: Deploy
  run: aws s3 sync ./dist s3://my-bucket/
  env:
    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET }}

â­ 3. Matrix Builds (A favorite interview question)

Runs a job across multiple environments.

Example:

strategy:
  matrix:
    node-version: [16, 18, 20]

âœ… Real-World Example 
â€œIn one of my projects, I used GitHub Actions to automate:
Running test cases on every push,
Building the frontend,
Creating a Docker image,
And pushing it to Docker Hub automatically.â€
This shows practical CI/CD knowledge.

âœ… GitHub Actions Deployment Example (Java + Docker + AWS EC2)
name: Deploy to EC2

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Build JAR
        run: mvn -B package

      - name: Build Docker Image
        run: docker build -t myapp .

      - name: Push to Docker Hub
        run: |
          echo ${{ secrets.DOCKER_PASS }} | docker login -u ${{ secrets.DOCKER_USER }} --password-stdin
          docker push myapp:latest

      - name: Deploy to EC2
        run: ssh ubuntu@${{ secrets.EC2_IP }} "docker pull myapp:latest && docker restart app"


##############################################
GitHub-Hosted Runners vs Self-Hosted Runners 
##############################################
1ï¸âƒ£ GitHub-Hosted Runners
Machines provided and managed by GitHub that automatically run your workflows.

ğŸ”„ Key Points:
â¤ Pre-configured with tools (Node, Java, Python, Docker, Git, etc.)
â¤ No setup or maintenance required
â¤ Supports Linux, Windows, macOS
â¤ Scales automatically
â¤ Limited runtime minutes depending on plan
â¤ Good for standard CI/CD, building, testing, linting, packaging

ğŸ”„ When to use:
â¤ Small/medium projects
â¤ Open-source repos
â¤ Standard build/testing
â¤ No special system-level installations needed

âœ… Example in workflow:
runs-on: ubuntu-latest

2ï¸âƒ£ Self-Hosted Runners
Your own machine/server/VM used to run GitHub Actions workflows.

ğŸ”„ Key Points:
â¤ You install GitHub runner software on your server
â¤ Full control over hardware, OS, installed tools, environment
â¤ No billing for CI minutes (runs on your machine)
â¤ Can be infinitely customized
â¤ Higher security for private/internal apps
â¤ Requires maintenance (patching, scaling, uptime)

ğŸ”„ When to use:
â¤ Need custom OS/tools (e.g., GPU, specific compiler, Android emulator)
â¤ Heavy builds (ML models, complex Java builds)
â¤ On-prem enterprise environments
â¤ Long-running workflows
â¤ Private cloud or custom infra
â¤ When GitHub-hosted runner limits cause problems

âœ… Example in workflow:
runs-on: self-hosted

ğŸ”„ Quick Comparison Table 
| Feature            | GitHub-Hosted Runner  | Self-Hosted Runner                            |
| ------------------ | --------------------- | --------------------------------------------- |
| **Infrastructure** | Provided by GitHub    | Your machine/server                           |
| **Setup**          | No setup needed       | Manual installation                           |
| **Cost**           | Free/minutes limit    | Runs on your hardware                         |
| **Scalability**    | Auto-scaled           | You manage scaling                            |
| **Customization**  | Limited               | Fully customizable                            |
| **Security**       | Good for public repos | Best for internal/private code                |
| **Performance**    | Standard              | Can be high-performance (GPU, more RAM, etc.) |


###############################################
ğŸ”· Host a Self-Hosted GitHub Runner on AWS 
###############################################
You can host a self-hosted runner on AWS EC2 (most common), but the same steps apply to ECS, EKS, or Lambda.

Hereâ€™s the simplest and most preferred method:
1ï¸âƒ£ Create an EC2 instance

Choose:
âœ”ï¸ Amazon Linux 2 or Ubuntu 20.04+
âœ”ï¸ Instance type â†’ t2.micro or t2.medium (depending on build needs)
âœ”ï¸ Storage â†’ 20â€“30 GB
âœ”ï¸ Allow SSH in security group - Edit Inbound Rules and Outbound Rules for HTTP, HTTPS, and SSH
 
âœ”ï¸ Install basic tools:
sudo yum update -y
sudo yum install git -y

2ï¸âƒ£ Go to Your GitHub Repository â†’ Settings â†’ Actions â†’ Runners
Path:
ğŸ”— Repository â†’ Settings â†’ Actions â†’ Runners â†’ New self-hosted runner

Choose:
âœ”ï¸ Linux
âœ”ï¸ x64

ğŸ”„  GitHub will show setup commands like:
# Create a folder for the runner
ğŸ”— mkdir actions-runner && cd actions-runner

# Download the latest runner package
ğŸ”— curl -o actions-runner-linux-x64.tar.gz -L https://github.com/actions/runner/releases/download/v2.X.Y/actions-runner-linux-x64-2.X.Y.tar.gz

# Extract it
ğŸ”— tar xzf ./actions-runner-linux-x64.tar.gz

# Configure the runner
ğŸ”— ./config.sh --url https://github.com/<username>/<repo> --token <TOKEN_FROM_GITHUB>
ğŸ‘‰ Paste these commands directly into your EC2 instance.

3ï¸âƒ£ Install Runner as a Service (MOST IMPORTANT)
This ensures the runner automatically restarts after reboot.
ğŸ”— sudo ./svc.sh install
ğŸ”— sudo ./svc.sh start

Check status:
ğŸ”— sudo ./svc.sh status

4ï¸âƒ£ Confirm Runner is Active
Go back to:
Repo â†’ Settings â†’ Actions â†’ Runners

You should see:
ğŸŸ¢ Runner is online

5ï¸âƒ£ Use the Self-Hosted Runner in Your Workflow
ğŸ”— Add this to your GitHub Actions workflow: inside the .github/workflows folder  

name: Build on Self-Hosted Runner

on: [push]

jobs:
  build:
    runs-on: self-hosted
    steps:
      - uses: actions/checkout@v3
      - name: Run build
        run: echo "Building on AWS EC2 self-hosted runner"

â­ Why Teams Use Self-Hosted Runners on AWS 
â¤ Custom environment: Java, Docker, Android SDK, ML libraries, GPU
â¤ Faster builds: choose high RAM / CPU EC2 instances
â¤ No GitHub usage limits
â¤ Secure private pipelines inside VPC
â¤ Needed for enterprise infrastructure


ğŸ”„ Bonus: AWS-specific Best Practices 
â¤ Use IAM Role on EC2 to access S3, ECR, ECS, Lambda, RDS, etc.
â¤ Use a VPC security group that allows only required traffic.

â¤ Install:
ğŸ”— sudo yum install docker -y
ğŸ”— sudo service docker start
ğŸ”— sudo usermod -aG docker ec2-user

â¤ Use auto-scaling group for multiple runners.
â¤ Use GitHub Runner Controller (GRC) on EKS for large orgs.


###############################
ğŸ”· Introduction to Containers
###############################
â¤ Containers are lightweight, isolated runtime environments that package an application along with its dependencies (libraries, configs, binaries).
â¤ They share the host OS kernel, unlike VMs which need a full guest OS.
â¤ They ensure â€œWorks on my machineâ€ issues disappear by providing a consistent environment across development, testing, and production.
â¤ Popular container runtime: Docker (most common), containerd, CRI-O, runc.


ğŸ”„ Problems With Virtual Machines
âŒ Problem 1ï¸âƒ£: Heavy Resource Usage
â¤ VMs include a full guest operating system â†’ large in size (GBs).
â¤ Heavy CPU/RAM consumption.

âŒ Problem 2ï¸âƒ£: Slow Boot Time
â¤ VMs take minutes to start because they boot a complete OS.

âŒ Problem 3ï¸âƒ£: Hard to Scale
â¤ Spinning up multiple VMs is costly and slow â†’ not ideal for microservices.

âŒ Problem 4ï¸âƒ£: Duplication of OS
â¤ Each VM has its own OS â†’ wastes disk space.

âŒ Problem 5ï¸âƒ£: Hard to Port
â¤ VM images are large â†’ difficult to move, copy, push/pull.

âŒ Problem 6ï¸âƒ£: Dev/Prod Inconsistency
â¤ Developers run different OS versions or dependencies â†’ deployment failures.


ğŸ”„ How Containers Solve Those Problems
1ï¸âƒ£ Lightweight (No Guest OS)
âœ”ï¸ Containers share the host OS kernel â†’ only app + its libraries are packaged.
âœ”ï¸ Much smaller than VMs (MBs instead of GBs).

2ï¸âƒ£ Fast Startup
âœ”ï¸ Containers start in seconds (no OS booting).

3ï¸âƒ£ Easy Scaling
âœ”ï¸ Spin up multiple containers instantly â†’ perfect for microservices + Kubernetes.

4ï¸âƒ£ Better Resource Utilization
More containers on same hardware vs VMs â†’ cheaper infrastructure.

5ï¸âƒ£ Portability
Same container image runs on:
  âœ”ï¸Dev laptop
  âœ”ï¸Test server
  âœ”ï¸Production
  âœ”ï¸Cloud

6ï¸âƒ£ Consistent Environments
âœ”ï¸ Application + dependencies packaged together â†’ no configuration drift.

 
ğŸ”„ Containers ğŸ†š VMs
| Feature      | Containers           | Virtual Machines              |
| ------------ | -------------------- | ----------------------------- |
| OS           | Share host OS kernel | Each has its own guest OS     |
| Startup time | Seconds              | Minutes                       |
| Size         | MBs                  | GBs                           |
| Isolation    | Process-level        | Full OS-level                 |
| Performance  | Near-native          | Some overhead                 |
| Scaling      | Very fast            | Slow                          |
| Use case     | Microservices, CI/CD | Strong isolation, legacy apps |


ğŸ”„ Drawbacks of Containers & How to Overcome Them
âŒ  Weaker Security (Shared Kernel)
â¤ If kernel is compromised â†’ all containers are at risk.

âœ… Solution:
â¤ Use VM isolation for untrusted workloads.
â¤ Use Kubernetes Pod Security Policies, AppArmor, SELinux.
â¤ Regular kernel patching.

âŒ Limited Isolation Compared to VMs
â¤ Containers share the same OS kernel.

âœ… Solution:
â¤ Use sandboxed containers (gVisor, Kata Containers).
â¤ Use hardened distros (Bottlerocket, Flatcar).

âŒ Complex Networking
â¤ Container networking (bridge, overlay, CNI plugins) is harder than VM networks.

âœ… Solution:
â¤ Use Kubernetes CNI (Calico, Flannel) for easier management.

âŒ Persistent Storage Challenges
â¤ Containers are ephemeral; data disappears on deletion.

âœ… Solution:
â¤ Use volumes: Docker volumes, Kubernetes PersistentVolumes.

âŒ Image Bloat
â¤ Large base images â†’ slow builds and pulls.

âœ… Solution:
â¤ Use slim images (alpine, debian-slim).
â¤ Multi-stage Docker builds.


ğŸ”„ Architecture of Containers
Container architecture consists of:

1ï¸âƒ£ User Layer
âœ”ï¸ Applications
âœ”ï¸ Libraries
âœ”ï¸ Dependencies
âœ”ï¸ Environment variables

2ï¸âƒ£ Container Image Layer
â¤ Immutable filesystem created by layering:
  âœ”ï¸ base image â†’ runtime â†’ dependencies â†’ source code
â¤ Stored in a registry (Docker Hub, ECR, GCR)

3ï¸âƒ£ Runtime Layer
Two main components:
1ï¸âƒ£ High-level runtime (Docker, containerd)
2ï¸âƒ£ Low-level runtime (runc) â€” responsible for creating the containerâ€™s namespaces & cgroups

4ï¸âƒ£ OS Layer
Kernel features used:
âœ”ï¸ Namespaces â†’ isolation
âœ”ï¸ cgroups â†’ resource limits
âœ”ï¸ UnionFS â†’ layered filesystem

5ï¸âƒ£ Host Layer
Host OS + hardware resources


ğŸ”„ Features of Containers
1ï¸âƒ£ Isolation
Each container runs independently.

2ï¸âƒ£ Portability
Runs anywhere (Linux, Windows, Cloud, Kubernetes).

3ï¸âƒ£ Lightweight
No OS included â†’ fast and small.

4ï¸âƒ£ Scalability
Scale up/down quickly.

5ï¸âƒ£ Immutability
Images are read-only; changes create new layers.

6ï¸âƒ£ Consistency
Same environment across all stages.

7ï¸âƒ£ Resource Efficiency
More apps per host vs VMs.

8ï¸âƒ£ Fast Deployment
Containers spin up in seconds.

9ï¸âƒ£ Loose Coupling
Perfect for microservices architecture.


ğŸ”„ Lifecycle of Containers
1ï¸âƒ£ Create
âœ”ï¸ Container is defined but not running.
ğŸ”— docker create image-name

2ï¸âƒ£ Start
âœ”ï¸ Container starts execution.
ğŸ”— docker start container-id

3ï¸âƒ£ Run
âœ”ï¸ Equivalent to create + start.
ğŸ”— docker run image-name

4ï¸âƒ£ Pause
âœ”ï¸ Temporarily pause execution.
ğŸ”— docker pause container-id

5ï¸âƒ£ Resume / Unpause
ğŸ”— docker unpause container-id

6ï¸âƒ£ Stop
âœ”ï¸ Gracefully stops the container (SIGTERM).
ğŸ”— docker stop container-id

7ï¸âƒ£ Kill
âœ”ï¸ Force stop (SIGKILL).
ğŸ”— docker kill container-id

8ï¸âƒ£ Restart
âœ”ï¸ Stop + start.
ğŸ”— docker restart container-id

9ï¸âƒ£ Remove
âœ”ï¸ Deletes the container.
ğŸ”— docker rm container-id


ğŸ”„ Files and Folders in containers base images
1ï¸âƒ£  /bin: contains binary executable files, such as the ls, cp, and ps commands.
2ï¸âƒ£ /sbin: contains system binary executable files, such as the init and shutdown commands.
3ï¸âƒ£ /etc: contains configuration files for various system services.
4ï¸âƒ£ /lib: contains library files that are used by the binary executables.
5ï¸âƒ£ /usr: contains user-related files and utilities, such as applications, libraries, and documentation.
6ï¸âƒ£ /var: contains variable data, such as log files, spool files, and temporary files.
7ï¸âƒ£ /root: is the home directory of the root user.

ğŸ”„ Files and Folders that containers use from host operating system
1ï¸âƒ£ The host's file system: Docker containers can access the host file system using bind mounts, which allow the container to read and write files in the host file system.

2ï¸âƒ£ The host's Networking stack: The host's networking stack is used to provide network connectivity to the container. Docker containers can be connected to the host's network directly or through a virtual network.

3ï¸âƒ£ The host's System calls: The host's kernel handles system calls from the container, which is how the container accesses the host's resources, such as CPU, memory, and I/O.

4ï¸âƒ£ Namespaces: Docker containers use Linux namespaces to create isolated environments for the container's processes. Namespaces provide isolation for resources such as the file system, process ID, and network.

5ï¸âƒ£ Control groups (cgroups): Docker containers use cgroups to limit and control the amount of resources, such as CPU, memory, and I/O, that a container can access.


###########
ğŸ”· Docker
###########
Docker is a containerization platform that provides easy way to containerize your applications, which means, using Docker you can build container images, run the images to create containers and also push these containers to container regestries such as DockerHub, Quay.io and so on.

In simple words, you can understand as containerization is a concept or technology and Docker Implements Containerization.

ğŸ”„ Docker LifeCycle
There are three important things,
1ï¸âƒ£ docker build -> builds docker images from Dockerfile
2ï¸âƒ£ docker run -> runs container from docker images
3ï¸âƒ£ docker push -> push the container image to public/private regestries to share the docker images.


ğŸ”„ Understanding the terminology
1ï¸âƒ£ Docker daemon
The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.

2ï¸âƒ£ Docker client
The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.

3ï¸âƒ£ Docker Desktop
Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and share containerized applications and microservices. Docker Desktop includes the Docker daemon (dockerd), the Docker client (docker), Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see Docker Desktop.

4ï¸âƒ£ Docker registries
âœ”ï¸ A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry.

âœ”ï¸ When you use the docker pull or docker run commands, the required images are pulled from your configured registry. When you use the docker push command, your image is pushed to your configured registry. Docker objects

âœ”ï¸ When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.

5ï¸âƒ£ Dockerfile
Dockerfile is a file where you provide the steps to build your Docker Image.

6ï¸âƒ£ Images
An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run.

You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.


ğŸ”„ INSTALL DOCKER
A very detailed instructions to install Docker are provide in the below link

https://docs.docker.com/get-docker/

For Demo,
1ï¸âƒ£ You can create an Ubuntu EC2 Instance on AWS and run the below commands to install docker.
ğŸ”— sudo apt update
ğŸ”— sudo apt install docker.io -y

2ï¸âƒ£ Start Docker and Grant Access
A very common mistake that many beginners do is, After they install docker using the sudo access, they miss the step to Start the Docker daemon and grant acess to the user they want to use to interact with docker and run docker commands.

3ï¸âƒ£ Always ensure the docker daemon is up and running.
A easy way to verify your Docker installation is by running the below command
ğŸ”— docker run hello-world

ğŸ‘‰ If the output says:
docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/create": dial unix /var/run/docker.sock: connect: permission denied.
See 'docker run --help'.

This can mean two things,
1ï¸âƒ£ Docker deamon is not running.
2ï¸âƒ£ Your user does not have access to run docker commands.

4ï¸âƒ£ Start Docker daemon
You use the below command to verify if the docker daemon is actually started and Active
ğŸ”— sudo systemctl status docker

5ï¸âƒ£ If you notice that the docker daemon is not running, you can start the daemon using the below command
ğŸ”— sudo systemctl start docker

6ï¸âƒ£ Grant Access to your user to run docker commands
To grant access to your user to run the docker command, you should add the user to the Docker Linux group. Docker group is create by default when docker is installed.
ğŸ”— sudo usermod -aG docker ubuntu

ğŸ‘‰ In the above command ubuntu is the name of the user, you can change the username appropriately.

âš¡ NOTE: : You need to logout and login back for the changes to be reflected.

Docker is Installed, up and running ğŸ¥³ğŸ¥³
Use the same command again, to verify that docker is up and running.

ğŸ”— docker run hello-world
âœ… Output should look like:
....
....
Hello from Docker!
This message shows that your installation appears to be working correctly.
...
...
