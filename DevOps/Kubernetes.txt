###################
ğŸ”· Orchestration
###################
Orchestration means automatically managing many things so that they work together correctly without manual effort.

ğŸ”„ Real-Life Analogy (Conceptual Understanding)
Think of an orchestra:
â¤ Many musicians (containers)
â¤ Different instruments (services)
â¤ A conductor (orchestrator)
â¤ Everyone plays at the right time, volume, and rhythm

ğŸ‘‰ Without a conductor â†’ chaos
ğŸ‘‰ With a conductor â†’ harmony


ğŸ”„ Why Orchestration is Needed
â¤ Modern applications: 
    âœ”ï¸ï¸ Are made of multiple microservices
    âœ”ï¸ Run on multiple machines
    âœ”ï¸ Must handle:
        âš¡ Failures
        âš¡ Traffic spikes
        âš¡ Updates without downtime

â¤ Manual management:
    âœ”ï¸ Is error-prone
    âœ”ï¸ Does not scale
    âœ”ï¸ Causes downtime
ğŸ‘‰ Orchestration removes manual work


ğŸ”„ What Exactly Does Orchestration Do?
Orchestration covers the full lifecycle of services:
1ï¸âƒ£ Deployment Management
â¤ Starts containers
â¤ Stops containers
â¤ Updates applications
â¤ Performs rolling updates

âœ… Example:
Deploy 3 instances of service-A

2ï¸âƒ£ Scheduling
â¤ Decides where to run containers
â¤ Picks best machine based on:
    âœ”ï¸ CPU
    âœ”ï¸ Memory
    âœ”ï¸ Availability
ğŸ‘‰ You donâ€™t choose machines manually.


3ï¸âƒ£ Scaling
â¤ Increase instances when traffic is high
â¤ Decrease instances when traffic is low

âœ… Example:
â¤ Scale from 3 â†’ 10 containers automatically


4ï¸âƒ£ Self-Healing
â¤ Restarts crashed containers
â¤ Recreates failed services
â¤ Moves workloads if a node fails
ğŸŒŸ System fixes itself automatically.


5ï¸âƒ£ Load Balancing
â¤ Distributes traffic across multiple instances
â¤ Prevents overload
âœ… Example:
Client â†’ Service â†’ Pod1 / Pod2 / Pod3


6ï¸âƒ£ Service Discovery
â¤ Services find each other automatically
â¤ No hardcoded IPs
âœ… Example: http://user-service


7ï¸âƒ£ Configuration & Secrets Management
â¤ Environment variables
â¤ Database credentials
â¤ API keys
ğŸ‘‰ Secure and centralized.


8ï¸âƒ£ Monitoring & Health Checks
â¤ Detects unhealthy services
â¤ Replaces bad instances


ğŸ”„ Orchestration in Container World
âŒ Without Orchestration (Docker Only)
âœ”ï¸ One host
âœ”ï¸ Manual scaling
âœ”ï¸ Manual restarts
âœ”ï¸ No health checks
âœ”ï¸ No coordination

âœ… With Orchestration (Kubernetes)
âœ”ï¸ Multi-node cluster
âœ”ï¸ Auto scaling
âœ”ï¸ Auto healing
âœ”ï¸ Load balancing
âœ”ï¸ Rolling updates
ğŸ‘‰ Kubernetes is a container orchestrator


âœ… Example: Without vs With Orchestration
ğŸ”„ Without vs With Orchestration
âŒ Without Orchestration
Server crashes â†’ App down
Traffic spike â†’ App slow
Container crashes â†’ Manual restart

âœ… With Orchestration
Server crashes â†’ App moved to another node
Traffic spike â†’ New containers created
Container crashes â†’ Auto restarted

7ï¸âƒ£ Orchestration vs Automation
| Automation            | Orchestration              |
| --------------------- | -------------------------- |
| Automates single task | Coordinates multiple tasks |
| Script-based          | System-based               |
| Local                 | Distributed                |
ğŸ‘‰ Orchestration is automation + coordination

ğŸ”„ Kubernetes as an Orchestrator
Kubernetes orchestrates:
âœ”ï¸ Pods
âœ”ï¸ Services
âœ”ï¸ Volumes
âœ”ï¸ Configurations
âœ”ï¸ Networking
ğŸ‘‰ You say: replicas: 3

âš¡ Kubernetes ensures:
âœ”ï¸ Always 3 running
âœ”ï¸ Restores if one fails
âœ”ï¸ Balances traffic
ğŸ‘‰ You declare what you want, Kubernetes decides how to achieve it


9ï¸âƒ£ Enterprise Perspective (Why Companies Need Orchestration)
â¤ High availability
â¤ Zero downtime
â¤ Cost optimization
â¤ Faster deployments
â¤ Resilient systems

âš¡ Thatâ€™s why:
â¤ Netflix
â¤ Google
â¤ Amazon
ğŸ‘‰ use orchestration platforms.


###############
ğŸ”· Kubernetes
###############
â“ What is Docker?
Docker is a containerization platform used to build, package, and run applications inside containers.


ğŸ”„ Key Role of Docker
â¤ Creates containers
â¤ Runs containers
â¤ Manages container lifecycle on a single machine
ğŸ‘‰ Docker is NOT an orchestration tool


ğŸ” Shortcomings of Docker
Docker is simple and minimalistic, which is good â€” but that simplicity causes limitations at scale.

1ï¸âƒ£ Single Host Limitation
âŒ Problem:
â¤ Docker by itself runs containers on one host
â¤ If the host machine goes down â†’ all containers go down

â—Impact:
â¤ No high availability
â¤ Not suitable for large distributed systems
ğŸ‘‰  Docker does not manage multiple machines


2ï¸âƒ£ Manual Scaling
âŒ Problem:
To scale containers, you must manually run:
ğŸ”— docker run ...
ğŸ”— docker run ...

â— Issues:
â¤ No auto-scaling
â¤ No CPU/memory-based scaling
â¤ Not practical in production


3ï¸âƒ£ No Self-Healing
âŒ Problem:
â¤ If a container crashes:
    âœ”ï¸ Docker does nothing automatically
    âœ”ï¸ Manual restart required

ğŸ”— docker ps
ğŸ”— docker restart <container>

â— Impact:
â¤ Downtime
â¤ Reliability issues


4ï¸âƒ£ No Built-in Load Balancing
âŒ Problem:
â¤ Docker does not distribute traffic automatically
â¤ You must configure external tools (Nginx, HAProxy)

â— Impact:
â¤ Extra configuration
â¤ Manual effort


5ï¸âƒ£ No Enterprise-Level Features
ğŸŒŸ Docker lacks:
    âœ”ï¸ Orchestration
    âœ”ï¸ Auto-scaling
    âœ”ï¸ Self-healing
    âœ”ï¸ Rolling updates
    âœ”ï¸ Service discovery
    âœ”ï¸ Fault tolerance
ğŸ‘‰ Docker is a container runtime, not a platform for large-scale distributed systems


ğŸ” Kubernetes
Kubernetes is a container orchestration platform that manages, scales, heals, and deploys containerized applications across a cluster of machines.
ğŸ‘‰ Kubernetes uses Docker (or other runtimes) underneath.


ğŸ”„ How Kubernetes Solves Dockerâ€™s Shortcomings
1ï¸âƒ£ Multi-Node Cluster Architecture
âŒ Docker Problem: Single host
âœ… Kubernetes Solution:
Kubernetes runs on a cluster of nodes

        Kubernetes Cluster
   ----------------------------
   | Master | Worker | Worker |
   ----------------------------

âœ”ï¸ Multiple machines
âœ”ï¸ High availability
âœ”ï¸ Fault tolerance
ğŸ‘‰ If one node fails â†’ workloads move to another node.


2ï¸âƒ£ Automatic Scaling (Replication Controller / ReplicaSet)
âŒ Docker Problem: Manual scaling
âœ… Kubernetes Solution:
â¤ ReplicaSet / Deployment
â¤ Maintains desired number of container replicas
ğŸ”— replicas: 3

ğŸ‘‰ If one container dies â†’ Kubernetes creates a new one

âš¡ Bonus:
â¤ Horizontal Pod Autoscaler (HPA)
â¤ Scales based on:
    âœ”ï¸ CPU
    âœ”ï¸ Memory
    âœ”ï¸ Metrics


3ï¸âƒ£Self-Healing
âŒ Docker Problem: No auto recovery
âœ… Kubernetes Solution: Kubernetes continuously monitors containers

â¤ If:
    âœ”ï¸ Container crashes â†’ restarted
    âœ”ï¸ Pod dies â†’ recreated
    âœ”ï¸ Node fails â†’ pods rescheduled
ğŸ‘‰ This is called self-healing


4ï¸âƒ£ Built-In Load Balancing
âŒ Docker Problem: No native load balancing
âœ… Kubernetes Solution:
â¤ Service object
â¤ Automatically load balances traffic across pods

Client â†’ Service â†’ Pod1 / Pod2 / Pod3
âœ”ï¸ No external tool required
âœ”ï¸ Internal & external traffic supported


ğŸ”„ Enterprise-Grade Orchestration Features
Kubernetes provides:
| Feature                  | Kubernetes |
| ------------------------ | ---------- |
| Rolling Updates          | âœ…         |
| Zero Downtime Deployment | âœ…         |
| Service Discovery        | âœ…         |
| Secrets & ConfigMaps     | âœ…         |
| Auto Scaling             | âœ…         |
| Self Healing             | âœ…         |
| RBAC & Security          | âœ…         |
ğŸ‘‰ This is why Kubernetes is production standard

ğŸ”„ Docker vs Kubernetes 
| Feature           | Docker           | Kubernetes         |
| ----------------- | ---------------- | ------------------ |
| Purpose           | Containerization | Orchestration      |
| Runs On           | Single host      | Multi-node cluster |
| Scaling           | Manual           | Automatic          |
| Self-Healing      | âŒ               | âœ…                |
| Load Balancing    | âŒ               | âœ…                |
| High Availability | âŒ               | âœ…                |
| Enterprise Ready  | âŒ               | âœ…                |

â— Kubernetes Does NOT Replace Docker
âœ”ï¸  Docker â†’ creates & runs containers
âœ”ï¸  Kubernetes â†’ manages & orchestrates containers
ğŸ“Œ They work together


############################
ğŸ”· Pods vs Node vs Cluster
############################
A Pod is the smallest deployable unit in Kubernetes that wraps one or more containers and shares network and storage.

â¤ Pod = wrapper around containers
â¤ Kubernetes never runs containers directly
â¤ Containers always run inside a Pod
â¤ Think of a Pod as: â€œA logical host for containersâ€


ğŸ”„ Key Characteristics of Pod
â¤ Smallest unit in Kubernetes
â¤ Usually contains one container
â¤ Containers in same Pod share:
    âœ”ï¸ IP address
    âœ”ï¸ Port space
    âœ”ï¸ Volumes


âœ… Pod Example
Pod
 â”œâ”€â”€ Container (Spring Boot App)
 â”œâ”€â”€ Shared Network (IP)
 â””â”€â”€ Shared Volume


ğŸ”„ Why Multiple Containers in a Pod
â¤ Sidecar pattern
â¤ Logging container
â¤ Monitoring container
âœ… Example:
â¤ App container
â¤ Log shipper container

Benifits of multiple containers in a Pod:
    âœ”ï¸ Shared lifecycle
    âœ”ï¸ Shared resources
    âœ”ï¸ Tight coupling


ğŸ”„ Node
A Node is a physical or virtual machine where Pods are scheduled and run.
Node = actual machine
Pods run on nodes
Node provides:
    âœ”ï¸ CPU
    âœ”ï¸ Memory
    âœ”ï¸ Network
    âœ”ï¸ Disk
ğŸ‘‰ Node is the worker machine

ğŸ”„ Components Inside a Node
Node
--------------------------------
| kubelet                       |
| container runtime             |
| kube-proxy                    |
--------------------------------
| Pods                          |
--------------------------------


ğŸ”„ Types of Nodes
| Node Type          | Purpose         |
| ------------------ | --------------- |
| Control Plane Node | Manages cluster |
| Worker Node        | Runs Pods       |


ğŸ”„ Node Failure
â¤ If node crashes:
    âœ”ï¸ Kubernetes moves pods to other nodes
    âœ”ï¸ Self-healing happens


ğŸ”„ Cluster
A Cluster is a group of nodes managed by Kubernetes that work together to run containerized applications.

â¤ Cluster = entire Kubernetes system
â¤ Includes:
    âœ”ï¸ Control plane
    âœ”ï¸ Multiple worker nodes
ğŸ‘‰ Users interact with the cluster, not individual nodes


ğŸ”„ Cluster Structure
Kubernetes Cluster
-------------------------------
| Control Plane               |
-------------------------------
| Worker Node | Worker Node  |
-------------------------------

ğŸ”„ Why Cluster?
âœ”ï¸ High availability
âœ”ï¸ Scalability
âœ”ï¸ Fault tolerance
âœ”ï¸ Load distribution


ğŸ”„ Relationship Between Pod, Node, and Cluster
Cluster
 â”œâ”€â”€ Node 1
 â”‚     â”œâ”€â”€ Pod A
 â”‚     â””â”€â”€ Pod B
 â”œâ”€â”€ Node 2
 â”‚     â””â”€â”€ Pod C
 â””â”€â”€ Node 3
       â””â”€â”€ Pod D

ğŸ”„ Hierarchy
Cluster > Node > Pod > Container


ğŸ”„ Pod vs Node vs Cluster
| Feature     | Pod           | Node             | Cluster             |
| ----------- | ------------- | ---------------- | ------------------- |
| What it is  | Smallest unit | Machine          | Collection of nodes |
| Contains    | Containers    | Pods             | Nodes               |
| Lifecycle   | Short-lived   | Long-lived       | Long-lived          |
| Managed by  | Kubernetes    | Kubernetes       | Kubernetes          |
| Scalability | Horizontal    | Add/remove nodes | Highly scalable     |


#####################
ğŸ”· K8s Architecture
#####################
1ï¸âƒ£ High-Level Overview
ğŸ“Œ What is Kubernetes Architecture?
Kubernetes follows a master-worker (control plane â€“ data plane) architecture, where:
    âœ”ï¸ Control Plane makes decisions
    âœ”ï¸ Worker Nodes run applications



ğŸ§± Cluster Structure
           Kubernetes Cluster
------------------------------------------------
|              Control Plane                   |
|----------------------------------------------|
| API Server | Scheduler | Controller Manager |
|            |           |                   |
|                 etcd                        |
------------------------------------------------
        |                 |                 |
------------------------------------------------
| Worker Node 1 | Worker Node 2 | Worker Node 3 |
------------------------------------------------
| kubelet | container runtime | kube-proxy     |
------------------------------------------------

ğŸ“Œ A cluster = Control Plane + Worker Nodes
2ï¸âƒ£ Control Plane (Master Node)
The control plane is the brain of Kubernetes.
ğŸ‘‰ It decides:
    âœ”ï¸ What should run
    âœ”ï¸ Where it should run
    âœ”ï¸ How to fix failures


ğŸ”„ API Server (kube-apiserver)
ğŸ“Œ Role: Entry point of Kubernetes cluster

ğŸ”„ Responsibilities:
â¤ Exposes Kubernetes REST API
â¤ Accepts all requests:
    âœ”ï¸ kubectl apply
    âœ”ï¸ kubectl get pods
â¤ Authenticates & validates requests
â¤ Talks to all other components
ğŸ“Œ All communication goes through API Server


2ï¸âƒ£ etcd (Key-Value Store)
ğŸ“Œ Role: Database of Kubernetes
â¤ Stores:
    âœ”ï¸ Cluster state
    âœ”ï¸ Pod info
    âœ”ï¸ Node info
    âœ”ï¸ ConfigMaps
    âœ”ï¸ Secrets
ğŸ“Œ If etcd is lost â†’ cluster state is lost
ğŸ“Œ etcd must be backed up


3ï¸âƒ£ Scheduler (kube-scheduler)
ğŸ“Œ Role: Decides where a pod should run

ğŸ”„ How Scheduler Works:
â¤ Looks for:
    âœ”ï¸ CPU availability
    âœ”ï¸ Memory availability
    âœ”ï¸ Node health
â¤ Chooses the best node
ğŸ“Œ Scheduler does not run pods, it only assigns nodes


4ï¸âƒ£ Controller Manager
ğŸ“Œ Role: Ensures desired state = actual state

Runs multiple controllers:
| Controller            | Function                |
| --------------------- | ----------------------- |
| Node Controller       | Detects node failure    |
| ReplicaSet Controller | Maintains pod count     |
| Deployment Controller | Handles rolling updates |
| Job Controller        | Manages batch jobs      |
ğŸ“Œ If something goes wrong â†’ controllers fix it


ğŸ”„ Worker Node Architecture (Where Apps Run)
Worker nodes are where your containers actually run.

ğŸ”„ Components Inside Worker Node
Worker Node
-----------------------------------
| kubelet                          |
| container runtime (Docker/CRI)   |
| kube-proxy                       |
-----------------------------------


1ï¸âƒ£ kubelet
ğŸ“Œ Role: Agent running on each worker node

ğŸ”„ Responsibilities:
â¤ Talks to API Server
â¤ Starts/stops containers
â¤ Reports node health
â¤ Ensures pod is running as expected
ğŸ“Œ kubelet = node manager


2ï¸âƒ£ Container Runtime
ğŸ“Œ Role: Runs containers

âœ… Examples:
Docker (older)
containerd
CRI-O

ğŸ“Œ Kubernetes does not run containers directly
ğŸ“Œ It uses CRI (Container Runtime Interface)


3ï¸âƒ£ kube-proxy
ğŸ“Œ Role: Handles networking & load balancing

ğŸ”„ Responsibilities:
â¤ Routes traffic to correct pods
â¤ Implements Services
â¤ Maintains iptables rules

ğŸ‘‰ kube-proxy enables:
ğŸ”— Service â†’ Pod1 / Pod2 / Pod3


ğŸ”„ How a Pod is Created (Request Flow)
âœ… Example:
ğŸ”— kubectl apply -f deployment.yaml

ğŸ”„ Step-by-Step Flow
1ï¸âƒ£ Request goes to API Server
2ï¸âƒ£ API Server validates request
3ï¸âƒ£ Object stored in etcd
4ï¸âƒ£ Scheduler selects node
5ï¸âƒ£ kubelet on selected node:
    âœ”ï¸ Pulls image
    âœ”ï¸ Starts container
6ï¸âƒ£ kube-proxy enables networking
7ï¸âƒ£ Pod becomes Running
ğŸ‘‰ Everything is automated


ğŸ”„ How Kubernetes Handles Failure (Recovery Flow)
1ï¸âƒ£ Container Crash
â¤ kubelet detects failure
â¤ Restarts container

2ï¸âƒ£ Pod Crash
â¤ Controller recreates pod

3ï¸âƒ£ Node Crash
â¤ Node Controller detects failure
â¤ Pods rescheduled to healthy nodes
ğŸ‘‰ This is self-healing

6ï¸âƒ£ Control Plane vs Worker Node
| Control Plane   | Worker Node       |
| --------------- | ----------------- |
| Manages cluster | Runs applications |
| Decision making | Execution         |
| API Server      | kubelet           |
| Scheduler       | container runtime |
| Controllers     | kube-proxy        |


ğŸ”„ Why This Architecture is Powerful
âœ”ï¸ Separation of concerns
âœ”ï¸ Highly scalable
âœ”ï¸ Fault tolerant
âœ”ï¸ Declarative system
âœ”ï¸ Cloud-agnostic


####################################
ğŸ”· Kubernetes Production Systems
####################################
A Kubernetes distribution is a packaged version of Kubernetes that includes the core Kubernetes components plus additional tools, configurations, security features, and integrations to make Kubernetes production-ready.

â¤ Kubernetes (upstream) = raw engine
â¤ Distribution = engine + safety + automation + enterprise tools

ğŸ“Œ Think of it like:
â¤ Linux kernel â†’ Ubuntu / RedHat / CentOS
â¤ Kubernetes core â†’ OpenShift / EKS / GKE / AKS

ğŸ”„ Why Distributions Exist?
âš¡ Upstream Kubernetes:
â¤ Hard to install
â¤ Hard to secure
â¤ Hard to upgrade
â¤ Hard to operate at scale

âš¡ Distributions solve:
â¤ Installation
â¤ Security
â¤ Monitoring
â¤ Networking
â¤ Upgrades
â¤ Cloud integration

ğŸ”„ Popular Kubernetes Distributions (Production Use)
Order you provided (âœ” correct) â€” explained one by one
1ï¸âƒ£ Kubernetes (Upstream / Vanilla K8s)
â¤ Core open-source Kubernetes
â¤ Maintained by CNCF
â¤ Used when:
    âœ”ï¸ Learning
    âœ”ï¸ Custom platforms
    âœ”ï¸ Internal tooling

â¤ Limitations:
    âœ”ï¸ No built-in UI
    âœ”ï¸ No enterprise security
    âœ”ï¸ Manual upgrades
    âœ”ï¸ Manual cluster management

2ï¸âƒ£ Red Hat OpenShift
Enterprise Kubernetes with opinionated security & developer tools
ğŸŒŸ Key Features
â¤ Built-in CI/CD
â¤ Built-in container registry
â¤ Strict security (non-root containers)
â¤ Web console
â¤ Enterprise support

ğŸŒŸ Used by
â¤ Banks
â¤ Telecom
â¤ Enterprises
ğŸ“Œ OpenShift = Kubernetes + enterprise guardrails

3ï¸âƒ£ Rancher
Kubernetes management platform (not just a cluster)
ğŸŒŸ Key Features
â¤ Manage 100s of clusters
â¤ Multi-cloud support
â¤ Centralized UI
â¤ RBAC across clusters

ğŸ“Œ Rancher does not replace Kubernetes, it manages Kubernetes

4ï¸âƒ£ VMware Tanzu
Enterprise Kubernetes for VMware environments
ğŸŒŸ Key Features
â¤ Deep vSphere integration
â¤ Kubernetes inside VMware infra
â¤ Hybrid cloud support

ğŸŒŸ Used by
Enterprises already using VMware

5ï¸âƒ£ EKS (Amazon Elastic Kubernetes Service)
Fully managed Kubernetes control plane by AWS

AWS manages
    âœ”ï¸ API server
    âœ”ï¸ etcd
    âœ”ï¸ Control plane HA
    âœ”ï¸ Security patches

You manage
    âœ”ï¸ Worker nodes
    âœ”ï¸ Pods
    âœ”ï¸ Networking
    âœ”ï¸ Scaling

6ï¸âƒ£ AKS (Azure Kubernetes Service)
Managed Kubernetes on Azure
ğŸŒŸ Key Features
    âœ”ï¸ Azure AD integration
    âœ”ï¸ Azure Monitor
    âœ”ï¸ Azure networking

7ï¸âƒ£ GKE (Google Kubernetes Engine)
Googleâ€™s managed Kubernetes (most mature)

Why GKE is specialâ“
    âœ”ï¸ Kubernetes created by Google
    âœ”ï¸ Best auto-scaling
    âœ”ï¸ Best networking
    âœ”ï¸ Very stable

ğŸ”„ Difference Between Kubernetes ğŸ†š EKS
| Feature          | Kubernetes (Upstream) | EKS           |
| ---------------- | --------------------- | ------------- |
| Control Plane    | You manage            | AWS manages   |
| Installation     | Manual                | Automated     |
| HA               | Manual                | Built-in      |
| Upgrades         | Manual                | One-click     |
| Security         | DIY                   | AWS IAM       |
| Cost             | Free                  | Pay AWS infra |
| Production-ready | Hard                  | Easy          |


ğŸ”„ How DevOps Engineers Manage Hundreds of Kubernetes Clusters?
They donâ€™t manage clusters manually â€” they automate everything.

ğŸŒŸ Tools Used
1ï¸âƒ£ kops (Kubernetes Operations)
ğŸ“Œ Used for:
    âœ”ï¸ Creating production clusters on AWS
    âœ”ï¸ Managing cluster lifecycle

Features:
    âœ”ï¸ Cluster creation
    âœ”ï¸ Auto-scaling
Upgrade support

2ï¸âƒ£ kubeadm
ğŸ“Œ Used for:
    âœ”ï¸ Bootstrapping Kubernetes clusters
    âœ”ï¸ Mostly for on-prem or custom infra

âŒ Not for managing 100s of clusters alone

3ï¸âƒ£ Rancher / OpenShift / Tanzu
ğŸ“Œ Centralized management:
    âœ”ï¸ Single dashboard
    âœ”ï¸ RBAC
    âœ”ï¸ Policies
    âœ”ï¸ Cluster visibility


################################################
ğŸ”· Installation of Kubernetes using KOPS on EC2
################################################
Kops (Kubernetes Operations) is a tool used to create, upgrade, and manage production-grade Kubernetes clusters on cloud providers like AWS.

â“ Why Kops Exists?
Installing Kubernetes manually on EC2 is:
    âœ”ï¸ Complex
    âœ”ï¸ Error-prone
    âœ”ï¸ Not production-safe

ğŸ“Œ Kops automates everything:
    âœ”ï¸ EC2 instances
    âœ”ï¸ VPC
    âœ”ï¸ Auto Scaling Groups
    âœ”ï¸ Load Balancers
    âœ”ï¸ IAM roles
    âœ”ï¸ etcd

Kubernetes control plane

ğŸ”„ Where Kops Fits in Kubernetes World
| Tool    | Purpose                          |
| ------- | -------------------------------- |
| kubeadm | Bootstrap Kubernetes (low-level) |
| kops    | Production cluster lifecycle     |
| EKS     | Fully managed Kubernetes         |
| Rancher | Manage many clusters             |

ğŸ“Œ Kops â‰  Kubernetes
ğŸ“Œ Kops = Cluster lifecycle manager

ğŸ”„ High-Level Architecture
AWS S3 (State Store)
      â†“
Kops CLI
      â†“
AWS APIs
      â†“
EC2 + VPC + ASG + ELB
      â†“
Kubernetes Cluster

ğŸ”„ Prerequisites
This approach installs Kubernetes using KOPS, which is a production-grade Kubernetes lifecycle management tool for AWS.
You can perform this either on:
â¤ An EC2 instance (recommended for AWS-based setup), or
â¤ Your personal laptop (Linux preferred).


ğŸ”„ Dependencies Required (Why they are needed)
1ï¸âƒ£ Python3: Required internally by AWS CLI and some automation scripts.
2ï¸âƒ£ AWS CLI: Allows your system to talk to AWS and create resources like EC2, VPC, S3, IAM, etc.
3ï¸âƒ£ kubectl: Command-line tool used to interact with the Kubernetes cluster once it is created.
4ï¸âƒ£ KOPS: The main tool that creates, updates, validates, and deletes Kubernetes clusters on AWS.


ğŸ”„ Installing kubectl (Updated Kubernetes Repository â€“ Change Highlighted)
ğŸ” What changed?
â¤ Earlier, kubectl was installed from deprecated Google repos
â¤ Now Kubernetes uses pkgs.k8s.io, which is the official, secure repository

âœ… Explanation of commands:
ğŸ”— sudo apt-get update
ğŸ”— sudo apt-get install -y ca-certificates curl apt-transport-https
ğŸ‘‰ These packages allow secure communication (HTTPS) and repository access.

ğŸ”— curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key \
| sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
â¤ Downloads Kubernetes signing key
â¤ Converts it into a secure keyring format (modern Ubuntu requirement)

ğŸ”— echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] \
https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' \
| sudo tee /etc/apt/sources.list.d/kubernetes.list
ğŸ‘‰ Adds Kubernetes v1.28 repository to your system

ğŸ”— sudo apt-get update
ğŸ”— sudo apt-get install -y kubectl
ğŸ‘‰ Installs kubectl safely from the official repo

ğŸ“Œ Why v1.28?
â¤ Stable
â¤ Production-tested
â¤ Compatible with recent KOPS releases


ğŸ”„ Installing AWS CLI (Ubuntu 24.04 Compatible)
ğŸ” What changed?
â¤ Older apt-based AWS CLI installs often break on Ubuntu 24.04
â¤ Snap-based installation is now recommended

ğŸ”— sudo snap install aws-cli --classic

â¤ Installs AWS CLI v2 (latest)
â¤ --classic allows full system access
ğŸ”— export PATH="$PATH:/home/ubuntu/.local/bin/"

ğŸ‘‰ Ensures AWS CLI is accessible from terminal


ğŸ”„ Installing KOPS (Latest Stable Release)
curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s \
https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64

â¤ Dynamically fetches latest stable KOPS version
â¤ Avoids hardcoding versions (production best practice)

ğŸ”— chmod +x kops-linux-amd64
ğŸ”— sudo mv kops-linux-amd64 /usr/local/bin/kops
ğŸ‘‰ Makes KOPS executable system-wide


ğŸ”„ IAM Permissions (Why they are required)
â¤ KOPS creates AWS infrastructure, so it must call AWS APIs.

Required permissions:
    âœ”ï¸ AmazonEC2FullAccess â†’ create EC2 instances
    âœ”ï¸ AmazonS3FullAccess â†’ store cluster state
    âœ”ï¸ IAMFullAccess â†’ create roles & policies
    âœ”ï¸ AmazonVPCFullAccess â†’ networking resources
ğŸ“Œ If you use Admin user, these are already included.

ğŸ”„ Configure AWS CLI
ğŸ”— aws configure

â¤ This command stores:
    âœ”ï¸ AWS Access Key
    âœ”ï¸ Secret Key
    âœ”ï¸ Default region (example: us-east-1)
    âœ”ï¸ Output format
ğŸ“Œ From this point onward, all KOPS commands interact with AWS automatically.


ğŸ”„ Create S3 Bucket (KOPS State Store)
ğŸ”— aws s3api create-bucket --bucket kops-abhi-storage --region us-east-1

â“ Why S3 is mandatory?
â¤ KOPS stores cluster configuration, secrets, certificates
â¤ Acts as the single source of truth for the cluster
â¤ Enables cluster updates, recovery, and deletion


ğŸ”„ Create the Kubernetes Cluster (Configuration Phase)
ğŸ”— kops create cluster \
--name=demok8scluster.k8s.local \
--state=s3://kops-abhi-storage \
--zones=us-east-1a \
--node-count=1 \
--node-size=t2.micro \
--master-size=t2.micro \
--master-volume-size=8 \
--node-volume-size=8

â“ What this command actually does:
â¤ Creates cluster definition only
â¤ Stores it in S3
â¤ Does NOT create EC2 instances yet

ğŸ“Œ .k8s.local indicates:
â¤ Gossip-based DNS
â¤ Suitable for learning/demo
â¤ No Route53 required


ğŸ”„ Edit Cluster Configuration (Very Important Step)
ğŸ”— kops edit cluster demok8scluster.k8s.local

â“ Why this step is emphasized?
â¤ Default KOPS config may create:
    âœ”ï¸ Multiple master nodes
    âœ”ï¸ Larger instance sizes
    âœ”ï¸ Extra networking components

ğŸ“Œ Editing ensures:
    âœ”ï¸ Free-tier friendly resources
    âœ”ï¸ Controlled AWS billing
    âœ”ï¸ Production tuning when required


ğŸ”„ Build the Cluster (Actual Infrastructure Creation)
kops update cluster demok8scluster.k8s.local --yes --state=s3://kops-abhi-storage

â“ What happens internally:
    âœ”ï¸ EC2 instances are launched
    âœ”ï¸ Kubernetes control plane is installed
    âœ”ï¸ Worker nodes join the cluster
    âœ”ï¸ IAM roles and networking are configured
    âœ”ï¸ kubeconfig is generated automatically
â³ Takes 5â€“15 minutes


ğŸ”„ Validate the Cluster
ğŸ”— kops validate cluster demok8scluster.k8s.local

â“ What this checks:
    âœ”ï¸ Master node health
    âœ”ï¸ Worker node readiness
    âœ”ï¸ Kubernetes API accessibility
    âœ”ï¸ DNS and networking

âœ… Expected output:
ğŸ‘‰ Your cluster demok8scluster.k8s.local is ready

#############################################
ğŸ”· Deploy application using Kubernetes Pods
#############################################
ğŸ”„ In Kubernetes, deploying an application means:
â¤ Running your application inside a container
â¤ Scheduling that container inside a Pod
â¤ Allowing Kubernetes to manage its lifecycle
ğŸ“Œ Pod is the smallest deployable unit in Kubernetes.


ğŸ”„ Prerequisites (Before deployment)
You must have:
â¤ A running Kubernetes cluster
â¤ kubectl configured and connected to the cluster
â¤ Your application available as a container image (Docker image)

âœ… Example image:
nginx
mycompany/spring-boot-app:1.0

ğŸ”„ Need of Pod
â¤ Wraps one or more containers
â¤ Provides:
    âœ”ï¸ Same network (IP & port)
    âœ”ï¸ Shared storage
    âœ”ï¸ Shared lifecycle
ğŸ“Œ Even if you run one container, Kubernetes still runs it inside a Pod.


ğŸ”„ Ways to deploy an application using Pods
ğŸ‘‰ Two common ways
1ï¸âƒ£ Imperative (Quick test)
2ï¸âƒ£ Declarative (Recommended / Production)

ğŸ”„ Imperative Way (Quick & Temporary)
kubectl run my-app-pod --image=nginx --port=80

â“What happens?
â¤ Kubernetes creates a Pod
â¤ Pulls the image
â¤ Starts the container

ğŸ“Œ Not recommended for production
â¤ No version control
â¤ Hard to manage changes


ğŸ”„ Declarative Way (Recommended)
You define a YAML file describing the Pod.

ğŸ”— Pod Definition File (pod.yaml)
apiVersion: v1
kind: Pod
metadata:
  name: my-app-pod
  labels:
    app: my-app
spec:
  containers:
  - name: my-container
    image: nginx
    ports:
    - containerPort: 80


âœ… Explanation of Each Section
1ï¸âƒ£ apiVersion & kind
â¤ Tells Kubernetes what type of object this is
â¤ Pod belongs to API version v1

2ï¸âƒ£ metadata
â¤ name: Unique name of the Pod
â¤ labels: Used for identification and grouping

3ï¸âƒ£ spec
â¤ Defines how the Pod should run

4ï¸âƒ£ containers
â¤ List of containers inside the Pod
â¤ image: Docker image to run
â¤ containerPort: Port exposed inside container
ğŸ“Œ Kubernetes does not expose this port outside automatically


ğŸ”„ Deploy the Pod
ğŸ”— kubectl apply -f pod.yaml

â“ What happens?
â¤ Kubernetes reads the YAML
â¤ Schedules the Pod on a Node
â¤ Pulls image and starts container


ğŸ”„ Verify Pod Status
ğŸ”— kubectl get pods

ğŸ‘‰ Expected states:
    âœ”ï¸ Pending
    âœ”ï¸ Running
    âœ”ï¸ CrashLoopBackOff (if error)


ğŸ”„ View Pod Details
ğŸ”— kubectl describe pod my-app-pod

ğŸ‘‰ Shows:
    âœ”ï¸ Node assignment
    âœ”ï¸ Container state
    âœ”ï¸ Events (errors if any)


ğŸ”„ Access the Application
â¤ Pods are not accessible from outside by default.

â¤ Temporary access (for testing):
ğŸ”— kubectl port-forward pod/my-app-pod 8080:80

â¤ Access:
ğŸ”— http://localhost:8080


ğŸ”„ Logs of the Application
ğŸ”— kubectl logs my-app-pod

If multiple containers:
ğŸ”— kubectl logs my-app-pod -c my-container


ğŸ”„ Why Pods are NOT used directly in Production?
Pods:
    âœ”ï¸ Die easily
    âœ”ï¸ No auto-restart guarantee
    âœ”ï¸ No scaling
    âœ”ï¸ No self-healing

ğŸ“Œ Thatâ€™s why we use:
    âœ”ï¸ Deployment
    âœ”ï¸ ReplicaSet
    âœ”ï¸ StatefulSet

ğŸ”„ Production Flow
Docker Image
   â†“
Deployment
   â†“
ReplicaSet
   â†“
Pods

ğŸ‘‰ Pods are created and managed automatically.

ğŸ”„ Pod Deletion
ğŸ”— kubectl delete pod my-app-pod
ğŸ‘‰ Pod is permanently removed.


#################################################
âš¡ kubectl cheatsheet to refer any commands
#################################################


###########################
ğŸ”· Kubernetes Deployment
###########################
A Deployment is a higher-level Kubernetes object used to:
â¤ Deploy applications
â¤ Keep them running
â¤ Scale them
â¤ Update them without downtime
ğŸ‘‰ You never manage Pods directly in production â€” Deployments do that for you.

â¤ Deployment = Manager of Pods
âœ… You tell Kubernetes:
    âœ”ï¸  Which image to run
    âœ”ï¸  How many replicas you want
    âœ”ï¸  How updates should happen
â¤ Kubernetes ensures that desired state = actual state.


â“What problems does Deployment solve?
ğŸŒŸ Without Deployment:
    âœ”ï¸ Pod crashes â†’ app goes down
    âœ”ï¸ No scaling
    âœ”ï¸ No rolling updates
    âœ”ï¸ Manual recovery

ğŸŒŸ With Deployment:
    âœ”ï¸ Pods auto-restart
    âœ”ï¸ Auto scaling
    âœ”ï¸ Rolling updates
    âœ”ï¸ Rollback support


ğŸ”„ Internal Working 
Deployment
   â†“
ReplicaSet
   â†“
Pods
   â†“
Containers

â¤ Deployment defines the desired state
â¤ ReplicaSet maintains pod count
â¤ Pods run containers


ğŸ”„  Difference: Container ğŸ†š Pod ğŸ†š Deployment
| Feature         | Container           | Pod                      | Deployment                   |
| --------------- | ------------------- | ------------------------ | ---------------------------- |
| Definition      | Application runtime | Wrapper for container(s) | Controller that manages Pods |
| Smallest unit   | âŒ                   | âœ…                        | âŒ                            |
| Runs directly   | Yes (Docker)        | Yes (K8s)                | No                           |
| Contains        | App + dependencies  | One or more containers   | Pod template                 |
| IP Address      | âŒ                   | âœ… (one per Pod)          | âŒ                            |
| Scaling         | âŒ                   | âŒ                        | âœ…                            |
| Self-healing    | âŒ                   | âŒ                        | âœ…                            |
| Rolling updates | âŒ                   | âŒ                        | âœ…                            |
| Rollback        | âŒ                   | âŒ                        | âœ…                            |
| Production use  | âŒ                   | âŒ                        | âœ…                            |


ğŸ”„ Container (Lowest Level)
â¤ Created using Docker/CRI
â¤ Contains:
    âœ”ï¸ Application
    âœ”ï¸ Libraries
    âœ”ï¸ Runtime dependencies
â¤ No networking or orchestration logic
ğŸ“Œ Container alone is not production-ready


ğŸ”„ Pod (Kubernetes Runtime Unit)
â¤ Smallest deployable unit in Kubernetes
â¤ Wraps:
    âœ”ï¸ One or more containers
â¤ Provides:
    âœ”ï¸ Shared IP
    âœ”ï¸ Shared storage
    âœ”ï¸ Shared lifecycle
ğŸ“Œ Pods are ephemeral (temporary)


ğŸ”„ Deployment (Production-Grade Object)
â¤ Manages Pods via ReplicaSet
â¤ Ensures:
    âœ”ï¸ Desired number of replicas
    âœ”ï¸ Zero-downtime deployments
    âœ”ï¸ Automatic restarts
    âœ”ï¸ Version rollback
ğŸ“Œ Most commonly used Kubernetes object


âœ… Example
ğŸ”— Deployment YAML (nginx)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80


ğŸ”„ What happens when you apply this?
ğŸ”— kubectl apply -f deployment.yaml

Kubernetes:
    âœ”ï¸ Creates a Deployment
    âœ”ï¸ Creates a ReplicaSet
    âœ”ï¸ Creates 3 Pods
    âœ”ï¸ Keeps them running always


ğŸ”„ Real-World Analogy 
| Kubernetes | Real Life        |
| ---------- | ---------------- |
| Container  | Machine          |
| Pod        | Room             |
| Deployment | Building manager |


ğŸ”‘ Takeaways
â¤ Container â†’ runs app
â¤ Pod â†’ runs container(s)
â¤ Deployment â†’ manages Pods
â¤ Never deploy Pods directly in production
â¤ Deployment = scaling + self-healing + rolling updates

 
#################################################################
ğŸŒŠ KUBERNETES SERVICES | DISCOVERY | LOAD BALANCING | NETWORKING
#################################################################
Kubernetes networking solves 4 fundamental problems:
1ï¸âƒ£ Pod-to-Pod communication
2ï¸âƒ£ Service discovery
3ï¸âƒ£ Load balancing
4ï¸âƒ£ External access to applications

Kubernetes follows these networking guarantees:
1ï¸âƒ£ Every Pod gets its own IP
2ï¸âƒ£ Pods can communicate without NAT
3ï¸âƒ£ Nodes can communicate with Pods directly
4ï¸âƒ£ Services provide stable networking abstraction


ğŸ”„ Kubernetes Services 
â“ Why Services Exist
Pods are:
    âœ”ï¸ Ephemeral (can die anytime)
    âœ”ï¸ Recreated with new IPs
    âœ”ï¸ Horizontally scalable
So you cannot rely on Pod IPs.
ğŸ‘‰ Service = Stable virtual IP + DNS name that represents a logical group of Pods.


ğŸ”„ What a Service Actually Is
A Kubernetes Service is:
    âœ”ï¸ A logical abstraction
    âœ”ï¸ Has a stable Cluster IP
    âœ”ï¸ Uses label selectors to track Pods
    âœ”ï¸ Forwards traffic to healthy Pods
ğŸ‘‰ Client â†’ Service (VIP) â†’ Pod A / Pod B / Pod C


ğŸ” Service â†’ Pod Mapping (Endpoints)
Behind the scenes:
    âœ”ï¸ Kubernetes creates an Endpoints object
    âœ”ï¸ Contains all Pod IPs that match labels

If a Pod dies:
    âœ”ï¸ Endpoint is removed automatically
    âœ”ï¸ Traffic never goes to dead Pods


ğŸ”„ Types of Kubernetes Services
1ï¸âƒ£ ClusterIP
âœ”ï¸ Exposes Service inside the cluster only
âœ”ï¸ No external access

Use cases
âœ”ï¸ Internal microservice communication
âœ”ï¸ Backend APIs
âœ”ï¸ Databases
ğŸ‘‰ frontend â†’ backend-service â†’ backend pods

â¤ Only accessible within cluster
â¤ Stable virtual IP
â¤ DNS name auto-generated


2ï¸âƒ£ NodePort
â¤ Exposes Service on every Nodeâ€™s IP
â¤ Fixed port range: 30000â€“32767
ğŸ‘‰ Client â†’ NodeIP:NodePort â†’ Service â†’ Pod

â—Problems
â¤ Exposes every node
â¤ Hard to manage ports
â¤ Not production-friendly

ğŸ”„ Used mainly for:
â¤ Testing
â¤ Quick debugging
â¤ Bare-metal clusters


3ï¸âƒ£ LoadBalancer
â¤ Creates an external cloud load balancer
â¤ Routes traffic to Service automatically
ğŸ‘‰ Client â†’ Cloud LB â†’ Node â†’ Pod

ğŸ”„ On AWS
â¤ Creates ALB or NLB
â¤ Integrated with AWS networking

ğŸ”„ Best for:
â¤ Production workloads
â¤ Internet-facing apps


4ï¸âƒ£ ExternalName
â¤ Maps Service to external DNS name
â¤ No proxying, no endpoints

âœ… Example: api-service â†’ api.external.com

âœ”ï¸ Used when: Consuming external APIs as services


5ï¸âƒ£ Headless Service (clusterIP: None)
â¤ No virtual IP
â¤ DNS returns all Pod IPs
db-0.db-service
db-1.db-service

Used for:
    âœ”ï¸ StatefulSets
    âœ”ï¸ Databases (MongoDB, Cassandra)
    âœ”ï¸ Custom client-side load balancing


ğŸ” Kubernetes Service Discovery (How Pods Find Each Other)
Kubernetes provides built-in DNS-based discovery.

1ï¸âƒ£ CoreDNS
â¤ Runs as a Deployment in kube-system
â¤ Automatically registers:
    âœ”ï¸ Services
    âœ”ï¸ Pods
    âœ”ï¸ Endpoints

DNS Naming Format
ğŸ”— service-name.namespace.svc.cluster.local

âœ… Example: backend.default.svc.cluster.local
Pods can simply use: http://backend


ğŸ”„ Discovery Flow
Pod â†’ DNS Query â†’ CoreDNS â†’ Service IP â†’ kube-proxy â†’ Pod


ğŸ”„ Kubernetes Load Balancing (Internal Mechanics)
1ï¸âƒ£Service Load Balancing (L4)
Kubernetes uses:
    âœ”ï¸ kube-proxy
    âœ”ï¸ iptables or IPVS

What happens:
    âœ”ï¸ Service IP is virtual
    âœ”ï¸ Traffic is NATed to real Pod IPs
    âœ”ï¸ Round-robin distribution
ğŸ‘‰ This is Layer 4 load balancing


2ï¸âƒ£ Ingress Load Balancing (L7)
Ingress handles:
    âœ”ï¸ HTTP routing
    âœ”ï¸ Path-based routing
    âœ”ï¸ Host-based routing
    âœ”ï¸ TLS termination
ğŸ‘‰ Client â†’ Ingress Controller â†’ Services â†’ Pods

Ingress Controllers:
    âœ”ï¸ NGINX
    âœ”ï¸ Traefik
    âœ”ï¸ AWS ALB Ingress Controller


3ï¸âƒ£ Client-Side Load Balancing
Used with:
    âœ”ï¸ Headless Services
    âœ”ï¸ gRPC
    âœ”ï¸ Stateful apps

Client receives:
    âœ”ï¸ List of Pod IPs
    âœ”ï¸ Chooses target itself


ğŸ”Kubernetes Networking Model 
ğŸ”„ Pod-to-Pod Networking
    âœ”ï¸ Each Pod gets unique IP
    âœ”ï¸ Pods communicate directly
    âœ”ï¸ No port mapping needed

Implemented using CNI plugins:
    âœ”ï¸ Calico
    âœ”ï¸ Flannel
    âœ”ï¸ Cilium
    âœ”ï¸ Weave


ğŸ”„ Node-to-Pod
Nodes can reach Pods directly via Pod CIDR.

ğŸ”„ Pod-to-External
â¤ Via Node NAT
â¤ Or via egress gateway
â¤ Controlled using Network Policies


ğŸ” Kubernetes Networking on AWS
ğŸ”„ AWS EKS Networking
â¤ Uses VPC CNI
â¤ Each Pod gets:
    âœ”ï¸ Real VPC IP
    âœ”ï¸ ENI attached to Node

Benefits:
    âœ”ï¸ Native AWS networking
    âœ”ï¸ Security Groups can apply to Pods
    âœ”ï¸ No overlay network

Trade-off:
    âœ”ï¸ IP exhaustion risk
    âœ”ï¸ Requires CIDR planning


ğŸ”„ Load Balancing on AWS EKS
| K8s Object             | AWS Resource  |
| ---------------------- | ------------- |
| Service (LoadBalancer) | NLB           |
| Ingress                | ALB           |
| NodePort               | EC2 Node Port |


ğŸ”Security in Kubernetes Networking
ğŸ”„ Network Policies
Control Pod-to-Pod traffic:
    âœ”ï¸ Ingress rules
    âœ”ï¸ Egress rules

âœ… Example:
    âœ”ï¸ Only frontend can call backend
    âœ”ï¸ Backend cannot access DB except on port 5432


ğŸ”„ Service Mesh
Tools like:
    âœ”ï¸ Istio
    âœ”ï¸ Linkerd

Provide:
    âœ”ï¸ mTLS
    âœ”ï¸ Traffic shaping
    âœ”ï¸ Observability
    âœ”ï¸ Canary deployments


ğŸ”„ End-to-End Request Flow
User
 â†“
DNS (Route53)
 â†“
AWS ALB (Ingress Controller)
 â†“
Kubernetes Service
 â†“
kube-proxy
 â†“
Pod (Container)


#######################
ğŸŒŠ KUBERNETES INGRESS
#######################
Before Ingress, Kubernetes exposed apps using:
    âœ”ï¸ NodePort â†’ ugly ports, poor security
    âœ”ï¸ Service type LoadBalancer â†’ one cloud LB per service (expensive, unscalable)

Ingress solves:
    âœ”ï¸ Centralized HTTP/HTTPS entry point
    âœ”ï¸ L7 (application-level) routing
    âœ”ï¸ TLS termination
    âœ”ï¸ Path & host based routing
    âœ”ï¸ One Load Balancer â†’ many services


ğŸ” Ingress Actually Is
Ingress is NOT a load balancer
Ingress is:
    âœ”ï¸ A set of rules
    âœ”ï¸ That defines how external HTTP/HTTPS traffic reaches Services
ğŸ‘‰ It requires an Ingress Controller to function.

Ingress (Rules)
        â†“
Ingress Controller (NGINX / ALB / Traefik)
        â†“
Kubernetes Services
        â†“
Pods


ğŸ” Core Components of Ingress
1ï¸âƒ£ Ingress Resource
A Kubernetes YAML object containing:
    âœ”ï¸ Host rules
    âœ”ï¸ Path rules
    âœ”ï¸ Backend services
    âœ”ï¸ TLS configuration


âœ… Example :
example.com/login  â†’ auth-service
example.com/cart   â†’ cart-service


2ï¸âƒ£ Ingress Controller
The controller:
    âœ”ï¸ Watches Ingress resources
    âœ”ï¸ Creates/configures a real load balancer or proxy
    âœ”ï¸ Routes traffic accordingly

Common controllers:
    âœ”ï¸ NGINX Ingress
    âœ”ï¸ AWS ALB Ingress Controller
    âœ”ï¸ Traefik
    âœ”ï¸ HAProxy
    âœ”ï¸ Istio Gateway
ğŸ‘‰ No controller = Ingress does nothing


ğŸ” Ingress vs Service LoadBalancer
| Aspect    | Ingress                  | Service (LoadBalancer) |
| --------- | ------------------------ | ---------------------- |
| OSI Layer | Layer 7                  | Layer 4                |
| Routing   | Host/path based          | No routing             |
| TLS       | Centralized              | Per service            |
| Cost      | One LB for many services | One LB per service     |
| Features  | Auth, WAF, rewrites      | Simple forwarding      |


ğŸ” Ingress Routing Capabilities (L7 Power)
Ingress supports:
ğŸ”„ Host-Based Routing
api.example.com     â†’ api-service
admin.example.com   â†’ admin-service


ğŸ”„ Path-Based Routing
/users     â†’ user-service
/payments  â†’ payment-service


ğŸ”„ Combined Rules
shop.example.com/cart â†’ cart-service


ğŸ” TLS & HTTPS in Ingress
Ingress can:
    âœ”ï¸ Ingress â†’ ALBTerminate TLS
    âœ”ï¸ Ingress â†’ ALBManage multiple certificates
    âœ”ï¸ Ingress â†’ ALBSupport HTTPS redirect

TLS Secrets:
    âœ”ï¸ Ingress â†’ ALBStored as Kubernetes Secrets
    âœ”ï¸ Ingress â†’ ALBMounted by Ingress Controller
Client (HTTPS)
   â†“
Ingress Controller (TLS Termination)
   â†“
Service (HTTP)


On AWS:
    âœ”ï¸ Ingress â†’ ALBTLS can terminate at ALB
    âœ”ï¸ Ingress â†’ ALBCertificates from ACM


ğŸ” Ingress Request Flow (End-to-End)
User
 â†“
DNS (Route53)
 â†“
External Load Balancer
 â†“
Ingress Controller
 â†“
Ingress Rules Evaluation
 â†“
Service (ClusterIP)
 â†“
kube-proxy
 â†“
Pod


ğŸ” Ingress on AWS (EKS Specific)
ğŸ”„ AWS ALB Ingress Controllr (Now: AWS Load Balancer Controller)
Mapping:âœ”ï¸ Ingress â†’ ALB    
| Kubernetes Object | AWS Resource       |
| ----------------- | ------------------ |
| Ingress           | ALB                |
| Ingress Rules     | ALB Listener Rules |
| Service           | Target Group       |
| Pod               | Target             |
Benefits:
    âœ”ï¸ Native AWS ALB features
    âœ”ï¸ WAF integration
    âœ”ï¸ IAM + Security Groups
    âœ”ï¸ Auto scaling ALB


ğŸ” Security in Ingress
ğŸ”„ Network Security
â¤ Security Groups (AWS)
â¤ Firewall rules
â¤ Private vs public ALB


ğŸ”„ Application Security
â¤ AWS WAF
â¤ Rate limiting
â¤ IP whitelisting
â¤ Auth (OIDC, Cognito)


ğŸ”„ Zero Trust Pattern
â¤ Ingress is only public entry
â¤ Services are ClusterIP
â¤ Pods never exposed directly


ğŸ” Ingress vs API Gateway
| Feature   | Ingress       | API Gateway               |
| --------- | ------------- | ------------------------- |
| Protocols | HTTP/HTTPS    | HTTP + WebSocket + REST   |
| Cost      | Lower         | Higher                    |
| Features  | Routing, TLS  | Auth, caching, throttling |
| Best for  | Microservices | Public APIs               |


ğŸ” Advanced Ingress Patterns
ğŸ”„ Blue-Green / Canary Deployments
â¤ Traffic splitting
â¤ Weighted routing
â¤ Zero-downtime releases


ğŸ”„ Multi-Ingress Controllers
â¤ Internal ingress (private)
â¤ External ingress (public)


ğŸ”„ Multi-Tenant Clusters
â¤ Namespace-specific ingress rules
â¤ RBAC isolation


ğŸ” Ingress Limitations
â¤ HTTP/HTTPS only (L7)
â¤ Not suitable for TCP/UDP (unless special config)
â¤ Controller-specific annotations (vendor lock-in)
â¤ Complex debugging without observability


ğŸ” Common Traps
â“ Is Ingress a load balancer?
âŒ No â€” it defines routing rules. Controller does load balancing.

â“ Can Ingress work without controller?
âŒ No.

â“ Why not use LoadBalancer service for everything?
âŒ Cost + no routing + poor scalability.


ğŸ” When to Use What
| Use Case               | Choose         |
| ---------------------- | -------------- |
| Single app             | LoadBalancer   |
| Multiple microservices | Ingress        |
| gRPC, HTTP routing     | Ingress        |
| TCP/UDP traffic        | NLB / NodePort |
| API management         | API Gateway    |


ğŸ” Final Mental Model
Ingress = Smart HTTP Traffic Director
Ingress Controller = Actual traffic handler


##############################################
ğŸ”· Kubernetes RBAC(Role Based Access Control)
##############################################
â“ Why RBAC Exists
In Kubernetes:
â¤ Many users, teams, CI/CD systems, controllers
â¤ All interact with Kubernetes API Server
â¤ API can:
    âœ”ï¸ Create Pods
    âœ”ï¸ Delete namespaces
    âœ”ï¸ Modify Secrets
    âœ”ï¸ Scale workloads

Without RBAC:
    âŒ Anyone with kubeconfig could do anything
    âŒ No isolation between teams
    âŒ No compliance or audit control

ğŸ‘‰ RBAC answers one question:
Who can do WHAT, on WHICH resource, and WHERE (namespace or cluster)?


ğŸ”„ RBAC in Kubernetes â€“ High-Level View
RBAC is enforced only at the API Server level.

User / ServiceAccount
        â†“
    API Request
        â†“
Authentication (Who are you?)
        â†“
Authorization (RBAC)
        â†“
 Admission Controllers
        â†“
      Action Allowed / Denied
RBAC = Authorization layer, not authentication.


ğŸ”„ Core RBAC Building Blocks
RBAC is built using 4 main objects:
| Object                 | Purpose                                     |
| ---------------------- | ------------------------------------------- |
| **Role**               | What actions are allowed (namespace-scoped) |
| **ClusterRole**        | What actions are allowed (cluster-wide)     |
| **RoleBinding**        | Who gets a Role (namespace-scoped)          |
| **ClusterRoleBinding** | Who gets a ClusterRole (cluster-wide)       |
Think in 2 dimensions:
    âœ”ï¸ Permission definition â†’ Role / ClusterRole
    âœ”ï¸ Permission assignment â†’ RoleBinding / ClusterRoleBinding


ğŸ”„ Roles & ClusterRoles
1ï¸âƒ£ Role (Namespace-Scoped)
    âœ”ï¸ Allowed verbs
    âœ”ï¸ On specific resources
    âœ”ï¸ Inside one namespace only

âœ… Example permissions:
    âœ”ï¸ get, list Pods
    âœ”ï¸ create ConfigMaps
    âœ”ï¸ update Deployments

Role cannot:
    âœ”ï¸ Access resources in other namespaces
    âœ”ï¸ Access cluster-wide resources (nodes, namespaces)


2ï¸âƒ£ ClusterRole (Cluster-Scoped)
    âœ”ï¸ Across all namespaces
    âœ”ï¸ Or for non-namespaced resources

âœ… Examples:
    âœ”ï¸ nodes
    âœ”ï¸ namespaces
    âœ”ï¸ persistentvolumes
    âœ”ï¸ cluster-wide read access
ğŸ‘‰ ClusterRole can also be used inside a namespace via RoleBinding.


5ï¸âƒ£ Verbs, Resources, API Groups

RBAC rules are defined using:
1ï¸âƒ£ Verbs (Actions)
get, list, watch
create, update, patch, delete

2ï¸âƒ£ Resources
pods
deployments
services
configmaps
secrets

3ï¸âƒ£ API Groups
âœ… Examples:
    âœ”ï¸ "" â†’ core (pods, services)
    âœ”ï¸ apps â†’ deployments, replicasets
    âœ”ï¸ batch â†’ jobs, cronjobs

RBAC is explicit:
    âŒ No wildcard access unless defined
    âŒ No implicit permissions


ğŸ”„ RoleBinding & ClusterRoleBinding (Permission Assignment)
1ï¸âƒ£ RoleBinding
â¤ Binds:
    âœ”ï¸ Role OR ClusterRole
â¤ To:
    âœ”ï¸ User
    âœ”ï¸ Group
    âœ”ï¸ ServiceAccount
â¤ Within one namespace
User â†’ Role â†’ Namespace


2ï¸âƒ£ ClusterRoleBinding
â¤ Binds:
    âœ”ï¸ ClusterRole
â¤ To:
    âœ”ï¸ User / Group / ServiceAccount
â¤ Across entire cluster
User â†’ ClusterRole â†’ Entire Cluster


ğŸ”„ Subjects in RBAC (WHO gets access)
RBAC can grant permissions to:
1ï¸âƒ£ Users
â¤ External identities
â¤ Managed by:
    âœ”ï¸ Certificates
    âœ”ï¸ IAM (EKS)
    âœ”ï¸ OIDC

2ï¸âƒ£ Groups
    âœ”ï¸ Logical grouping of users
    âœ”ï¸ Used heavily in enterprises

3ï¸âƒ£ ServiceAccounts
Used by:
    âœ”ï¸ Pods
    âœ”ï¸ Controllers
    âœ”ï¸ CI/CD pipelines
ğŸ‘‰ Pods never authenticate as users â€” only as ServiceAccounts


ğŸ”„ ServiceAccounts & RBAC
Every Pod:
    âœ”ï¸ Uses a ServiceAccount
    âœ”ï¸ Default: default ServiceAccount (dangerous)

Flow:
Pod
 â†“
ServiceAccount
 â†“
RBAC Permissions
 â†“
API Server

Best practice:
    âœ”ï¸ One ServiceAccount per workload
    âœ”ï¸ Least privilege roles


ğŸ”„ Namespaced vs Cluster-Scoped Resources
| Resource Type    | Examples                    |
| ---------------- | --------------------------- |
| Namespace-Scoped | Pods, Services, Deployments |
| Cluster-Scoped   | Nodes, Namespaces, PVs      |
RBAC rule must match resource scope, otherwise:
âŒ Permission denied


ğŸ”„ Kubernetes RBAC Request Flow (Internal)
kubectl apply deployment.yaml
 â†“
API Server receives request
 â†“
Identifies user / service account
 â†“
RBAC checks:
   - Role / ClusterRole rules
   - RoleBinding / ClusterRoleBinding
 â†“
If allowed â†’ proceed
If denied â†’ 403 Forbidden
ğŸ‘‰ RBAC is deny by default.


ğŸ”„ RBAC in AWS EKS (Real World)
1ï¸âƒ£ Authentication vs Authorization
| Layer          | AWS EKS         |
| -------------- | --------------- |
| Authentication | AWS IAM         |
| Authorization  | Kubernetes RBAC |
IAM user â†’ mapped via aws-auth ConfigMap â†’ Kubernetes user/group â†’ RBAC

2ï¸âƒ£ IAM + RBAC Mapping Flow
IAM User / Role
 â†“
aws-auth ConfigMap
 â†“
Kubernetes User / Group
 â†“
RBAC Rules
ğŸ‘‰ IAM â‰  RBAC
ğŸ‘‰ IAM just gets you inside the cluster.


ğŸ”„ Common RBAC Patterns (Production)
1ï¸âƒ£ Namespace Isolation (Multi-Team)
Each team gets:
    âœ”ï¸ Own namespace
    âœ”ï¸ Own Role
    âœ”ï¸ Own RoleBinding

Teams cannot:
    âœ”ï¸ Access other namespaces
    âœ”ï¸ Modify cluster resources


2ï¸âƒ£ Read-Only Access
Used for:
    âœ”ï¸ Developers
    âœ”ï¸ Auditors
    âœ”ï¸ Monitoring tools

Permissions:
    âœ”ï¸ get, list, watch only


3ï¸âƒ£ CI/CD Pipeline Access
CI/CD needs:
    âœ”ï¸ Deploy workloads
    âœ”ï¸ Update images
    âœ”ï¸ Read logs

Grant:
    âœ”ï¸ create/update deployments
    âœ”ï¸ no access to secrets unless required


4ï¸âƒ£ Admin vs Power User
| Role       | Permissions            |
| ---------- | ---------------------- |
| Admin      | Full cluster access    |
| Power User | Namespace-wide control |
| Developer  | App resources only     |
| Viewer     | Read-only              |


ğŸ”„ RBAC Best Practices
âœ”ï¸ Principle of least privilege
âœ”ï¸ Never use cluster-admin casually
âœ”ï¸ Avoid using default ServiceAccount
âœ”ï¸ Separate human access & workload access
âœ”ï¸ Use Groups instead of individual users
âœ”ï¸ Audit RBAC using kubectl auth can-i
âœ”ï¸ Rotate credentials regularly


ğŸ”„Common Mistakes & Traps
âŒ Binding cluster-admin to everyone
âŒ Giving secrets access unnecessarily
âŒ Using ClusterRoleBinding when RoleBinding is enough
âŒ Forgetting namespace scope
âŒ Debugging RBAC without kubectl auth can-i


ğŸ”„ RBAC vs ABAC
| Feature            | RBAC       | ABAC            |
| ------------------ | ---------- | --------------- |
| Control            | Role-based | Attribute-based |
| Flexibility        | Moderate   | High            |
| Complexity         | Low        | Very High       |
| Kubernetes Support | Native     | Deprecated      |
ğŸ‘‰ Kubernetes uses RBAC only in modern clusters.


ğŸ”„ Mental Model 
Role / ClusterRole = WHAT you can do
RoleBinding / ClusterRoleBinding = WHO can do it & WHERE


####################################################
ğŸŒŠ KUBERNETES CUSTOM RESOURCES | CUSTOM CONTROLLER
####################################################
Kubernetes is not just a container orchestrator â€”
ğŸ‘‰ It is a distributed control plane framework.

CRDs + Controllers are how Kubernetes itself is built
(and how YOU extend it like native Kubernetes features).

ğŸ”„ Why Custom Resources Exist (The Core Motivation)

Out of the box, Kubernetes understands objects like:
    âœ”ï¸ Pod
    âœ”ï¸ Deployment
    âœ”ï¸ Service
    âœ”ï¸ ConfigMap

But real systems need higher-level abstractions:
    âœ”ï¸ Databases
    âœ”ï¸ Message queues
    âœ”ï¸ ML jobs
    âœ”ï¸ Workflows
    âœ”ï¸ Feature flags
    âœ”ï¸ Business-specific infrastructure

Without CRDs:
    âŒ External scripts
    âŒ Ad-hoc automation
    âŒ No Kubernetes-native lifecycle
    âŒ No kubectl support
ğŸ‘‰ CRDs allow you to teach Kubernetes new object types


ğŸ”„ What is a Custom Resource (CR)?
A new Kubernetes API object, defined by YOU, stored in etcd, and managed like native resources.

âœ… Example:
Database
Cache
KafkaCluster
Application
BackupPolicy

ğŸ‘‰ Once created:
    âœ”ï¸ kubectl get database
    âœ”ï¸ kubectl describe application
    âœ”ï¸ Stored in etcd
    âœ”ï¸ Versioned
    âœ”ï¸ RBAC-controlled
    âœ”ï¸ Watchable


ğŸ”„ What is a CRD (CustomResourceDefinition)?
A CRD is the schema + registration of your custom object.

ğŸ‘‰ Think of it as:
â€œI am telling Kubernetes:
this is a new resource type, and this is how it looks.â€

CRD defines:
    âœ”ï¸ Name
    âœ”ï¸ API group
    âœ”ï¸ Version
    âœ”ï¸ Schema (OpenAPI)
    âœ”ï¸ Scope (Namespaced / Cluster)


ğŸ”„ CRD vs CR
| Term | Meaning                      |
| ---- | ---------------------------- |
| CRD  | Blueprint / type definition  |
| CR   | Actual instance of that type |

ğŸ‘‰ Analogy:
Class â†’ CRD
Object â†’ CR


ğŸ”„ What Happens After Creating a CRD?
â¤ Once CRD is applied: kubectl apply -f crd.yaml
â¤ Kubernetes API Server:
    âœ”ï¸ Registers new API endpoint
    âœ”ï¸ Stores CRs in etcd
    âœ”ï¸ Exposes them via REST

BUT ğŸš¨
âš ï¸ Kubernetes does NOT know:
    âœ”ï¸ What action to take
    âœ”ï¸ How to reconcile state
    âœ”ï¸ What this resource means
ğŸ‘‰ This is where Controllers come in.


ğŸ”„ What is a Controller?
A control loop that watches resources and continuously reconciles desired state vs actual state.

Kubernetes itself is built on controllers:
    âœ”ï¸ Deployment Controller
    âœ”ï¸ ReplicaSet Controller
    âœ”ï¸ Node Controller
    âœ”ï¸ Job Controller
Custom Controller = Your logic, your rules


ğŸ”„ Why CRDs Alone Are Not Enough
CRDs:
    âœ”ï¸ Define data
    âœ”ï¸ Not behavior

Controllers:
    âœ”ï¸ Define behavior
    âœ”ï¸ Enforce state
    âœ”ï¸ Perform actions
ğŸ‘‰ CRD + Controller = Operator Pattern


ğŸ”„ The Operator Pattern
An Operator is a Custom Controller that manages a CRD using domain knowledge

âœ… Examples:
    âœ”ï¸ MySQL Operator
    âœ”ï¸ Kafka Operator
    âœ”ï¸ Prometheus Operator
    âœ”ï¸ ArgoCD Operator

Operator responsibilities:
    âœ”ï¸ Provision resources
    âœ”ï¸ Configure apps
    âœ”ï¸ Handle upgrades
    âœ”ï¸ Perform healing
    âœ”ï¸ Manage lifecycle


ğŸ”„ CRD + Controller High-Level Flow
User applies Custom Resource
        â†“
API Server stores CR in etcd
        â†“
Controller watches CR events
        â†“
Controller reconciles state
        â†“
Creates / updates native resources
        â†“
Updates CR status
ğŸ‘‰ This loop runs continuously.


ğŸ”„ Reconciliation Loop (Heart of Controllers)
Every controller follows this logic:

while true:
  desired_state = spec
  actual_state = cluster
  if desired != actual:
     take action

ğŸ‘‰ Key ideas:
    âœ”ï¸ Event-driven + periodic
    âœ”ï¸ Idempotent
    âœ”ï¸ Self-healing
    âœ”ï¸ Eventually consistent


ğŸ”„ Anatomy of a Custom Resource
A CR has two major sections:
1ï¸âƒ£ spec
    âœ”ï¸ Desired state
    âœ”ï¸ Provided by user

2ï¸âƒ£ status
    âœ”ï¸ Actual state
    âœ”ï¸ Updated by controller
ğŸ‘‰ This separation is fundamental to Kubernetes design.


ğŸ”„ How Controllers Interact with CRs
Controller actions:
    â¤ Watch CR events (add/update/delete)
    â¤ Read .spec
    â¤ Create/update:
        âœ”ï¸ Pods
        âœ”ï¸ Deployments
        âœ”ï¸ Services
        âœ”ï¸ ConfigMaps
        âœ”ï¸ Secrets
    â¤ Update .status

Controller NEVER:
    â¤ Modifies .spec
    â¤ Trusts current state blindly


ğŸ”„ Watching Resources (Under the Hood)
Controllers use:
    âœ”ï¸ Kubernetes Watch API
    âœ”ï¸ Informers
    âœ”ï¸ Shared caches

Benefits:
    âœ”ï¸ Low API load
    âœ”ï¸ Fast reactions
    âœ”ï¸ Event-based logic
ğŸ‘‰ This is how Kubernetes scales to thousands of objects.


ğŸ”„ Custom Controller vs Admission Controller
| Feature              | Custom Controller   | Admission Controller       |
| -------------------- | ------------------- | -------------------------- |
| Purpose              | Reconcile state     | Validate/mutate requests   |
| Runs                 | After object exists | Before object is persisted |
| Can create resources | Yes                 | No                         |
| Long-running         | Yes                 | No                         |
ğŸ‘‰ They solve different problems.


ğŸ”„ Example Use Cases (Real World)
1ï¸âƒ£ Application CRD
â¤ Define app config once
â¤ Auto-create:
    âœ”ï¸ Deployment
    âœ”ï¸ Service
    âœ”ï¸ Ingress
    âœ”ï¸ HPA

2ï¸âƒ£ Database CRD
â¤ Create StatefulSets
â¤ Handle backups
â¤ Rotate secrets
â¤ Scale replicas

3ï¸âƒ£ Infrastructure CRD
â¤ Provision cloud resources
â¤ Sync external state
â¤ Enforce policies


ğŸ”„ Error Handling & Resilience
Controllers must:
    âœ”ï¸ Retry safely
    âœ”ï¸ Be idempotent
    âœ”ï¸ Handle partial failures
    âœ”ï¸ Never assume success
Failures are normal â€” reconciliation handles it.


ğŸ”„ Versioning & Upgrades 
CRDs support:
    âœ”ï¸ Multiple API versions
    âœ”ï¸ Conversion webhooks
    âœ”ï¸ Backward compatibility

âœ… Example: v1alpha1 â†’ v1beta1 â†’ v1

Controller must:
    âœ”ï¸ Support migrations
    âœ”ï¸ Handle old CRs gracefully


ğŸ”„ RBAC & Security for Controllers
Controllers need permissions to:
    âœ”ï¸ Watch CRs
    âœ”ï¸ Create/update native resources

Best practice:
    âœ”ï¸ Dedicated ServiceAccount
    âœ”ï¸ Least privilege RBAC
    âœ”ï¸ No cluster-admin


ğŸ”„ Common Mistakes 
âŒ Business logic in CRD schema
âŒ Controllers modifying .spec
âŒ Non-idempotent reconciliation
âŒ Overusing ClusterScoped CRDs
âŒ No status updates
âŒ Tight coupling to cloud providers


ğŸ”„ Mental Model
CRD = New Kubernetes noun
Controller = Verb that makes it real
Or:
Kubernetes doesnâ€™t â€œdoâ€ anything by itself â€”
Controllers make the cluster move.


###########################
ğŸŒŠ ConfigMaps and Secrets
###########################
Kubernetes separates application code from configuration & sensitive data.
ConfigMaps and Secrets are the official mechanism for this separation.

ğŸ”„ Why ConfigMaps & Secrets Exist
Without them:
    âœ”ï¸ Config hard-coded in images
    âœ”ï¸ Credentials baked into code
    âœ”ï¸ Rebuild image for config change
    âœ”ï¸ High risk of leaks
    âœ”ï¸ Poor environment portability
Kubernetes philosophy: Build once, deploy everywhere

ConfigMaps & Secrets solve:
    âœ”ï¸ Externalized configuration
    âœ”ï¸ Environment-specific values
    âœ”ï¸ Secure handling of sensitive data
    âœ”ï¸ Runtime configuration changes


2ï¸âƒ£ ConfigMap
A Kubernetes object used to store non-sensitive configuration data in key-value or file form.
âœ… Examples:
    âœ”ï¸ App configs
    âœ”ï¸ Feature flags
    âœ”ï¸ URLs
    âœ”ï¸ Logging levels
    âœ”ï¸ Application properties

Stored in:
    âœ”ï¸ etcd (plain text)


3ï¸âƒ£ Secret
A Kubernetes object used to store sensitive information like credentials, tokens, and keys.
âœ… Examples:
    âœ”ï¸ Database passwords
    âœ”ï¸ API keys
    âœ”ï¸ TLS certificates
    âœ”ï¸ OAuth tokens

Important truth:
âš ï¸ Kubernetes Secrets are NOT encrypted by default
They are:
    âœ”ï¸ Base64-encoded
    âœ”ï¸ Stored in etcd

Encryption at rest must be explicitly enabled.

4ï¸âƒ£ ConfigMap vs Secret (Conceptual Difference)
| Aspect         | ConfigMap          | Secret          |
| -------------- | ------------------ | --------------- |
| Purpose        | Non-sensitive data | Sensitive data  |
| Stored in etcd | Plain text         | Base64          |
| Encryption     | âŒ Default no      | âŒ Default no  |
| Size limit     | ~1MB               | ~1MB            |
| Access control | RBAC               | RBAC (critical) |
| Visibility     | Safe to log        | Must never log  |


ğŸ”„ Why Secrets Are Separate from ConfigMaps
Reasons:
    âœ”ï¸ Security semantics
    âœ”ï¸ RBAC granularity
    âœ”ï¸ Tooling expectations
    âœ”ï¸ Audit & compliance
    âœ”ï¸ Integration with secret managers
ğŸ‘‰ Even though technically similar, intent matters.


ğŸ”„ Internal Storage & Security Implications
1ï¸âƒ£ etcd Reality
Both ConfigMaps & Secrets:
âœ”ï¸ Stored in etcd
âœ”ï¸ Replicated
âœ”ï¸ Backed up

Risk:
âœ”ï¸ etcd compromise = full cluster data leak

Production best practice:
âœ”ï¸ Enable encryption at rest
âœ”ï¸ Restrict etcd access
âœ”ï¸ Avoid secrets in logs


ğŸ”„ How Pods Consume ConfigMaps & Secrets
They are never injected automatically.
Pods explicitly consume them via:
1ï¸âƒ£ Environment variables
2ï¸âƒ£ Volume mounts


ğŸ”„ Using as Environment Variables
ConfigMap / Secret
 â†“
Environment Variables
 â†“
Application reads from ENV

Characteristics:
    âœ”ï¸ Simple
    âœ”ï¸ Popular
    âœ”ï¸ Immutable after pod start
    âœ”ï¸ Requires pod restart for updates

Risk:
âŒ Environment variables may leak via:
    âœ”ï¸ Logs
    âœ”ï¸ Stack traces
    âœ”ï¸ Debug tools


ğŸ”„ Using as Volumes
Flow:
ConfigMap / Secret
 â†“
Mounted as files
 â†“
Application reads files

Characteristics:
    âœ”ï¸ Dynamic updates (ConfigMap)
    âœ”ï¸ Files updated automatically
    âœ”ï¸ Better for large configs
    âœ”ï¸ Common for TLS certs

Secret volumes:
    âœ”ï¸ Stored in tmpfs (in memory)
    âœ”ï¸ Not written to disk by default


ğŸ”Ÿ Update Behavior
| Method        | Update Propagation        |
| ------------- | -----------------------   |
| Env vars      | âŒ Requires Pod restart  |
| Volume mount  | âœ… Updates automatically |
| SubPath mount | âŒ No updates            |
ğŸ‘‰ Many production outages happen due to misunderstanding this.


ğŸ”„ Immutability & Stability
ConfigMaps & Secrets can be:
âœ”ï¸ Mutable (default)
âœ”ï¸ Immutable (recommended)

Immutable:
âœ”ï¸ Prevents accidental changes
âœ”ï¸ Improves performance
âœ”ï¸ Encourages versioned configs

Pattern:
app-config-v1
app-config-v2


ğŸ”„ Secrets & RBAC
Secrets are:
    âœ”ï¸ Namespace-scoped
    âœ”ï¸ Protected by RBAC

Golden rule: If a Pod can read a Secret, it can exfiltrate it
Never:
âŒ Mount secrets broadly
âŒ Give list access to secrets


ğŸ”„ ServiceAccount & Secret Access
Pods access secrets via:
    âœ”ï¸ Their ServiceAccount
    âœ”ï¸ RBAC rules

Best practice:
    âœ”ï¸ Dedicated ServiceAccount per app
    âœ”ï¸ Minimal secret access


ğŸ”„ Types of Kubernetes Secrets
Built-in types:
    âœ”ï¸ Opaque (generic)
    âœ”ï¸ kubernetes.io/dockerconfigjson
    âœ”ï¸ kubernetes.io/tls
    âœ”ï¸ service-account-token

Each type:
    âœ”ï¸ Has semantic meaning
    âœ”ï¸ Used by controllers automatically


ğŸ”„ Secret Rotation & Lifecycle
Challenges:
    âœ”ï¸ Secrets are static
    âœ”ï¸ No auto-rotation
    âœ”ï¸ Manual updates required

Production approaches:
    âœ”ï¸ External Secret Managers
    âœ”ï¸ Short-lived credentials
    âœ”ï¸ Sidecar reloaders


ğŸ”„ External Secret Management
Kubernetes native Secrets are often not enough.

Common integrations:
    âœ”ï¸ AWS Secrets Manager
    âœ”ï¸ HashiCorp Vault
    âœ”ï¸ Azure Key Vault
    âœ”ï¸ GCP Secret Manager

Patterns:
    âœ”ï¸ Sync secrets into Kubernetes
    âœ”ï¸ Inject secrets at runtime
    âœ”ï¸ Avoid storing secrets in etcd


ğŸ”„ Common Anti-Patterns
âŒ Secrets in ConfigMaps
âŒ Secrets in Git
âŒ Logging environment variables
âŒ Sharing secrets across namespaces
âŒ Using default ServiceAccount
âŒ Hardcoding secret names


ğŸ”„ Debugging & Visibility
Useful commands:
    âœ”ï¸ Check mounts
    âœ”ï¸ Inspect pod env (carefully)
    âœ”ï¸ Review RBAC permissions
    âœ”ï¸ Audit secret access

Never:
âŒ Print secrets in logs
âŒ Share kubeconfig with secret access


ğŸ”„ Real-World Production Patterns
1ï¸âƒ£ App Configuration
    âœ”ï¸ ConfigMaps for static config
    âœ”ï¸ Immutable configs
    âœ”ï¸ Rolling restarts

2ï¸âƒ£ Credentials
    âœ”ï¸ Secrets for credentials
    âœ”ï¸ External secret manager
    âœ”ï¸ Short-lived tokens

3ï¸âƒ£ TLS
    âœ”ï¸ Secrets as volumes
    âœ”ï¸ Auto-rotation via controller


ğŸ”„ Mental Model
ConfigMaps = What the app needs to know
Secrets = What the app must protect
Or:
Code â†’ Image
Config â†’ ConfigMap
Secrets â†’ Secret


####################################################
ğŸŒŠ KUBERNETES MONITORING USING PROMETHEUS & GRAFANA
####################################################
ğŸ”„ Why Kubernetes Monitoring Is Hard
Kubernetes is:
    âœ”ï¸ Dynamic (pods come & go)
    âœ”ï¸ Distributed
    âœ”ï¸ Declarative
    âœ”ï¸ Event-driven


Traditional monitoring assumptions break:
âŒ Fixed IPs
âŒ Static hosts
âŒ Long-running processes


Monitoring must be:
âœ”ï¸ Service-aware
âœ”ï¸ Label-driven
âœ”ï¸ Pull-based
âœ”ï¸ Time-series focused


â“ Why Prometheus for Kubernetes
Prometheus fits Kubernetes by design.

Core Reasons:
    âœ”ï¸ Native Kubernetes service discovery
    âœ”ï¸ Label-based metrics
    âœ”ï¸ Pull model (simpler security)
    âœ”ï¸ Powerful query language (PromQL)
    âœ”ï¸ CNCF standard
Kubernetes + Prometheus is the default industry stack.


ğŸ”„ High-Level Architecture
Application Pods
   â†“ expose /metrics
Prometheus
   â†“ scrape & store
Time-Series Database (TSDB)
   â†“ query
Grafana
   â†“ visualize
Users / Alerts


ğŸ”„ Prometheus Core Concepts 
1ï¸âƒ£ Metrics
â¤ Prometheus collects numeric time-series metrics.
â¤ Each metric = metric_name{label1="value", label2="value"} value timestamp
â¤ Labels are the soul of Prometheus.

ğŸŒŸ Metric Types
| Type      | Meaning                             |
| --------- | ----------------------------------- |
| Counter   | Always increases (requests, errors) |
| Gauge     | Can go up/down (CPU, memory)        |
| Histogram | Latency buckets                     |
| Summary   | Client-side latency                 |


ğŸ”„Prometheus Pull Model (Why It Matters)
Prometheus scrapes metrics instead of receiving pushes.

Advantages:
    âœ”ï¸ No agent credentials on apps
    âœ”ï¸ Easier firewall rules
    âœ”ï¸ Better failure detection
    âœ”ï¸ Kubernetes-friendly
Scrape failure = signal itself.


ğŸ”„ How Prometheus Discovers Pods (Kubernetes SD)
Prometheus uses:
    âœ”ï¸Kubernetes API
    âœ”ï¸Service discovery

It discovers:
    âœ”ï¸Pods
    âœ”ï¸Services
    âœ”ï¸Endpoints
    âœ”ï¸Nodes

Based on:
    âœ”ï¸Labels
    âœ”ï¸Annotations
No hardcoded IPs.


ğŸ”„ The Role of Exporters
â¤ Most systems donâ€™t expose Prometheus metrics by default.
â¤ Exporters translate metrics into Prometheus format.

Common Exporters
| Exporter           | Purpose                 |
| ------------------ | ----------------------- |
| Node Exporter      | Node CPU, memory, disk  |
| kube-state-metrics | Kubernetes object state |
| cAdvisor           | Container metrics       |
| App Exporter       | App-specific metrics    |


ğŸ”„ kube-state-metrics
kube-state-metrics:
    âœ”ï¸ Reads Kubernetes objects
    âœ”ï¸ Converts desired state into metrics

âœ… Examples:
    âœ”ï¸ Pod status
    âœ”ï¸ Replica counts
    âœ”ï¸ Deployment availability
It does NOT read actual resource usage.


ğŸ”„ Prometheus Storage Model (TSDB)
Prometheus stores:
    âœ”ï¸ Time-series data
    âœ”ï¸ On local disk
    âœ”ï¸ In blocks

Key traits:
    âœ”ï¸ High write throughput
    âœ”ï¸ Limited retention
    âœ”ï¸ Not meant for long-term storage

Production solution:
    âœ”ï¸ Thanos
    âœ”ï¸ Cortex
    âœ”ï¸ Mimir


ğŸ”„ PromQL (Query Language)
â¤ Query time-series data
â¤ Aggregate metrics
â¤ Filter by labels
â¤ Compute rates & percentiles
â¤ Monitoring is useless without good queries.


ğŸ”„ Alerting with Prometheus
Prometheus evaluates:
    â¤ Alert rules
    â¤ On intervals

Alerts go to:
    â¤ Alertmanager

Alertmanager:
    â¤ Groups alerts
    â¤ Deduplicates
    â¤ Routes to:
        âœ”ï¸ Slack
        âœ”ï¸ PagerDuty
        âœ”ï¸ Email
Alerting â‰  Monitoring
Alerting is monitoringâ€™s output.


ğŸ”„ Grafana (Visualization Layer)
Grafana:
    âœ”ï¸ Queries Prometheus
    âœ”ï¸ Visualizes data
    âœ”ï¸ Builds dashboards

Grafana does:
    âœ”ï¸ Dashboards
    âœ”ï¸ Alerts (optional)
    âœ”ï¸ Annotations
    âœ”ï¸ Correlations
Grafana does NOT store metrics.


ğŸ”„ Why Prometheus + Grafana Together
| Prometheus       | Grafana        |
| ---------------- | -------------- |
| Data collection  | Visualization  |
| Time-series DB   | Dashboard UI   |
| Alert evaluation | Alert display  |
| Metrics storage  | Query frontend |
They are complementary, not competitors.


ğŸ”„ Monitoring Layers in Kubernetes
1ï¸âƒ£ Cluster Level
â¤ Node health
â¤ API server
â¤ Scheduler

2ï¸âƒ£ Namespace Level
â¤ Resource usage
â¤ Pod availability

3ï¸âƒ£ Application Level
â¤ Request rate
â¤ Latency
â¤ Error rate (RED metrics)


ğŸ”„ The Golden Signals (Production Monitoring)
Every service should track:
1ï¸âƒ£ Latency
2ï¸âƒ£ Traffic
3ï¸âƒ£ Errors
4ï¸âƒ£ Saturation
Prometheus excels at this.


ğŸ”„ Kubernetes Metrics You Should Always Monitor
â¤ Pod restarts
â¤ CrashLoopBackOff
â¤ Node CPU/memory pressure
â¤ API server latency
â¤ etcd health
â¤ Replica mismatch
â¤ HPA scaling behavior


ğŸ”„ Prometheus Operator (Production Standard)
â¤ Manual Prometheus is painful.
â¤ Prometheus Operator provides:
    âœ”ï¸ CRDs
    âœ”ï¸ Automation
    âœ”ï¸ Best practices

Key CRDs:
    âœ”ï¸ Prometheus
    âœ”ï¸ ServiceMonitor
    âœ”ï¸ PodMonitor
    âœ”ï¸ Alertmanager
This is how Prometheus is deployed in real clusters.


ğŸ”„ ServiceMonitor & PodMonitor
They define:
    âœ”ï¸ What to scrape
    âœ”ï¸ How to scrape
    âœ”ï¸ Which labels
â¤ Controller creates Prometheus config dynamically.
â¤ No static YAML edits.


ğŸ”„ Scaling Prometheus 
Challenges:
    âœ”ï¸ High cardinality
    âœ”ï¸ Storage limits
    âœ”ï¸ Single-node design

Solutions:
    âœ”ï¸ Sharding
    âœ”ï¸ Thanos
    âœ”ï¸ Remote write
Monitoring systems must be monitored themselves.


ğŸ”„ Security Considerations
â¤ Protect /metrics endpoints
â¤ Limit RBAC access
â¤ Secure Prometheus UI
â¤ Avoid leaking labels with secrets
â¤ Encrypt metrics traffic


ğŸ”„ Common Production Mistakes
âŒ High-cardinality labels (user_id, request_id)
âŒ Alerting on symptoms, not causes
âŒ No retention strategy
âŒ Ignoring Prometheus itself
âŒ Dashboards without alerts


ğŸ”„ Mental Model
Prometheus = Truth collector
Grafana = Truth visualizer
Or:
Kubernetes emits signals â†’ Prometheus records â†’ Grafana tells the story


########################################
ğŸŒŠ Installation of PROMETHEUS & GRAFANA
########################################
There are 3 common ways to install Prometheus & Grafana:
| Method              | Usage                 |
| ------------------- | --------------------- |
| Manual YAML         | Learning, internals   |
| Helm Charts         | Real-world production |
| Prometheus Operator | Enterprise / scalable |
ğŸ‘‰ Best practice:
Helm + Prometheus Operator (kube-prometheus-stack)


1ï¸âƒ£ Pre-Requisites
Before installing anything, ensure:

ğŸ” Kubernetes Cluster
â¤ Minikube / Kind / EKS / GKE / AKS
â¤ kubectl configured
ğŸ”— Check: kubectl get nodes


ğŸ” Helm Installed (Mandatory)
Helm is the package manager for Kubernetes.
ğŸ”— Check: helm version

ğŸ”— If not installed: curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash


ğŸ”„ Why Helm + Operator 
Manual Prometheus setup requires:
    âœ”ï¸ ConfigMaps
    âœ”ï¸ Scrape configs
    âœ”ï¸ Reload logic
    âœ”ï¸ TLS
    âœ”ï¸ Scaling logic

Operator abstracts this via:
    âœ”ï¸ CRDs
    âœ”ï¸ Controllers
    âœ”ï¸ Auto-generated configs
Operator = Kubernetes way of managing complex systems


â“ What We Are Installing (High-Level)
Using kube-prometheus-stack, we get:
| Component          | Purpose                    |
| ------------------ | -------------------------- |
| Prometheus         | Metrics collection         |
| Alertmanager       | Alert routing              |
| Grafana            | Visualization              |
| Node Exporter      | Node metrics               |
| kube-state-metrics | Cluster state              |
| CRDs               | ServiceMonitor, PodMonitor |


2ï¸âƒ£ Add Helm Repository
Add the Prometheus community repo:
ğŸ”— helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
ğŸ”— helm repo update

â“ Why?
â¤ Helm charts live here
â¤ Versioned & maintained by CNCF community


5ï¸âƒ£ Create Namespace (Best Practice)
ğŸ”— kubectl create namespace monitoring

â“ Why?
â¤ Isolation
â¤ RBAC control
â¤ Cleaner management


6ï¸âƒ£ Install Prometheus & Grafana 
Install using kube-prometheus-stack:
ğŸ”— helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring
ğŸ‘‰ This single command installs everything.


ğŸ” What Helm Actually Creates
Behind the scenes, Helm creates:

1ï¸âƒ£ CRDs
â¤ Prometheus
â¤ ServiceMonitor
â¤ PodMonitor
â¤ Alertmanager

2ï¸âƒ£ Controllers
â¤ Prometheus Operator

3ï¸âƒ£ Workloads
â¤ Prometheus StatefulSet
â¤ Grafana Deployment
â¤ Alertmanager StatefulSet
â¤ Node Exporter DaemonSet
â¤ kube-state-metrics Deployment

4ï¸âƒ£ Services
â¤ Prometheus service
â¤ Grafana service
â¤ Alertmanager service

5ï¸âƒ£ ConfigMaps & Secrets
â¤ Dashboards
â¤ Datasources
â¤ Credentials


7ï¸âƒ£ Verify Installation
ğŸ”— Check pods: kubectl get pods -n monitoring

âœ… Output You should see:
    âœ”ï¸ prometheus-*
    âœ”ï¸ grafana-*
    âœ”ï¸ alertmanager-*
    âœ”ï¸ node-exporter-*
    âœ”ï¸ kube-state-metrics-*
If all are Running, installation is successful.


8ï¸âƒ£ Access Prometheus UI
Prometheus service is ClusterIP by default.
Option 1ï¸âƒ£: Port Forward (Learning / Local)
ğŸ”— kubectl port-forward svc/monitoring-kube-prometheus-prometheus 9090:9090 -n monitoring
ğŸ”— Open browser: http://localhost:9090


9ï¸âƒ£ Access Grafana UI
ğŸ”— Get Grafana Password: kubectl get secret monitoring-grafana -n monitoring \
  -o jsonpath="{.data.admin-password}" | base64 --decode
ğŸ”— Username: admin
ğŸ”— Port Forward Grafana: kubectl port-forward svc/monitoring-grafana 3000:80 -n monitoring
ğŸ”— Open:http://localhost:3000


ğŸ” Grafana Datasource 
Prometheus datasource is:
    âœ”ï¸ Auto-configured
    âœ”ï¸ Linked internally
You donâ€™t need to add it manually.


ğŸ” Prebuilt Dashboards
Grafana comes with:
    âœ”ï¸ Node dashboards
    âœ”ï¸ Kubernetes cluster dashboards
    âœ”ï¸ Pod & namespace views

These dashboards are injected via:
    âœ”ï¸ ConfigMaps
    âœ”ï¸ Grafana sidecar
This is why Operator setup is powerful.


ğŸ” Metrics Start Flowing (Internal Flow)
Node Exporter â†’ Prometheus
kube-state-metrics â†’ Prometheus
App /metrics â†’ Prometheus
Prometheus â†’ TSDB
Grafana â†’ Prometheus â†’ Dashboards

ğŸ‘‰ Prometheus discovers targets using:
    âœ”ï¸ ServiceMonitor
    âœ”ï¸ PodMonitor
    âœ”ï¸ Kubernetes API


ğŸ” Installing on Cloud (EKS/GKE/AKS)
Same steps apply.

Additional considerations:
    âœ”ï¸ PersistentVolume for Prometheus
    âœ”ï¸ Ingress for Grafana
    âœ”ï¸ Authentication (OIDC)
    âœ”ï¸ Long-term storage (Thanos)


ğŸ” Common Installation Mistakes
âŒ Forgetting Helm
âŒ Installing without namespace
âŒ Not enough node memory
âŒ Assuming Grafana auto-exposes externally
âŒ Not checking CRDs installed


ğŸ” Uninstall Cleanly
ğŸ”— helm uninstall monitoring -n monitoring
ğŸ”— kubectl delete namespace monitoring
ğŸ”— CRDs may remain: kubectl delete crd prometheuses.monitoring.coreos.com



#########################################
ğŸŒŠ Architecture of  PROMETHEUS & GRAFANA
#########################################
Prometheus + Grafana together form a metrics-based observability system, where:
    âœ”ï¸ Prometheus = collector + time-series database
    âœ”ï¸ Grafana = query + visualization layer
They are loosely coupled, which is a deliberate architectural choice.


ğŸ” High-Level System Architecture
Applications / Exporters
        â†“  (/metrics)
    Prometheus Server
        â†“
   Time Series Database
        â†“
     PromQL Engine
        â†“
       Grafana
        â†“
      Users / Alerts
ğŸ‘‰ Prometheus is stateful
ğŸ‘‰ Grafana is stateless


ğŸ” Prometheus Architecture 
Prometheus is a single-binary system, but internally it has multiple subsystems.
1ï¸âƒ£Service Discovery Layer
Purpose:
    âœ”ï¸ Find targets dynamically
    âœ”ï¸ No static IPs

Sources:
    âœ”ï¸ Kubernetes API
    âœ”ï¸ EC2
    âœ”ï¸ Consul
    âœ”ï¸ Static configs

In Kubernetes:
ServiceMonitor / PodMonitor
        â†“
Kubernetes API
        â†“
Target List


2ï¸âƒ£ Scrape Manager
Responsibilities:
    âœ”ï¸ Periodically scrape targets
    âœ”ï¸ Default interval: 15s
    âœ”ï¸ Handle failures

Scrape lifecycle:
GET /metrics
Parse text format
Attach labels

ğŸ‘‰ Pull failure itself becomes a metric.


3ï¸âƒ£ Label Processor
Every metric is enriched with:
    âœ”ï¸ Namespace
    âœ”ï¸ Pod
    âœ”ï¸ Node
    âœ”ï¸ Service
    âœ”ï¸ Job

Labels allow:
    âœ”ï¸ Aggregation
    âœ”ï¸ Filtering
    âœ”ï¸ Multi-dimensional queries
ğŸ‘‰ Bad labels = scaling disaster.


4ï¸âƒ£ Time Series Database (TSDB)
Prometheus TSDB stores:
    âœ”ï¸ Time-series data
    âœ”ï¸ On local disk

Storage format:
    âœ”ï¸ Write-ahead log (WAL)
    âœ”ï¸ Immutable blocks

Optimized for:
    âœ”ï¸ High write throughput
    âœ”ï¸ Short-to-medium retention

Not designed for:
âŒ Long-term archival
âŒ Heavy joins


5ï¸âƒ£ Query Engine (PromQL)
Executes:
    âœ”ï¸ Instant queries
    âœ”ï¸ Range queries

Works on:
    âœ”ï¸ In-memory indexes
    âœ”ï¸ Disk blocks

Expensive queries:
    âœ”ï¸ High cardinality
    âœ”ï¸ Large time ranges


6ï¸âƒ£ Alerting Engine
Evaluates:
    âœ”ï¸ Alert rules
    âœ”ï¸ On schedule

Produces:
    âœ”ï¸ Firing alerts
    âœ”ï¸ Resolved alerts

Alerts are forwarded to:
    âœ”ï¸ Alertmanager


7ï¸âƒ£ HTTP API
Used by:
    âœ”ï¸ Grafana
    âœ”ï¸ CLI tools
    âœ”ï¸ Remote write/read
    âœ”ï¸ Federation
This API makes Prometheus extensible.


ğŸ” Alertmanager Architecture
â¤ Alertmanager is a separate service.
â¤ Responsibilities:
    âœ”ï¸ Deduplicate alerts
    âœ”ï¸ Group related alerts
    âœ”ï¸ Silence noise
    âœ”ï¸ Route to channels

Prometheus
   â†“
Alertmanager
   â†“
Slack / PagerDuty / Email

ğŸ‘‰ Prometheus detects
ğŸ‘‰ Alertmanager communicates


4ï¸âƒ£ Grafana Architecture
Grafana is purely a presentation & query layer.

1ï¸âƒ£ Data Source Layer
Grafana supports:
    âœ”ï¸ Prometheus
    âœ”ï¸ Loki
    âœ”ï¸ Elasticsearch
    âœ”ï¸ CloudWatch

For Prometheus:
    âœ”ï¸ Uses HTTP API
    âœ”ï¸ Sends PromQL queries

Grafana does not cache metrics permanently.


2ï¸âƒ£ Query Engine 
Responsibilities:
    âœ”ï¸ Translate dashboard panels into queries
    âœ”ï¸ Handle variables
    âœ”ï¸ Combine multiple queries
Grafana is query-heavy but stateless.


3ï¸âƒ£ Dashboard Engine
Dashboards are:
    âœ”ï¸ JSON models
    âœ”ï¸ Versionable
    âœ”ï¸ Shareable
Supports:
    âœ”ï¸ Templating
    âœ”ï¸ Variables
    âœ”ï¸ Drilldowns
    âœ”ï¸ Annotations

4ï¸âƒ£ Rendering Engine
Turns metrics into:
    âœ”ï¸ Graphs
    âœ”ï¸ Tables
    âœ”ï¸ Heatmaps
    âœ”ï¸ Alerts
Visualization logic is separate from data retrieval.


5ï¸âƒ£ Authentication & Authorization
Grafana supports:
    âœ”ï¸ Local users
    âœ”ï¸ OAuth
    âœ”ï¸ SAML
    âœ”ï¸ LDAP
Auth is independent of Prometheus.


ğŸ” Prometheus + Grafana Interaction
User opens dashboard
        â†“
Grafana sends PromQL
        â†“
Prometheus executes query
        â†“
Time-series returned
        â†“
Grafana renders visualization
ğŸ‘‰ Grafana never stores metrics.


ğŸ” Kubernetes-Specific Architecture (Production)
Node Exporter (DaemonSet)
kube-state-metrics
Application Exporters
        â†“
   Prometheus (StatefulSet)
        â†“
     TSDB + WAL
        â†“
   Alertmanager
        â†“
     Grafana (Deployment)
ğŸ‘‰ Installed & managed via:
    âœ”ï¸ Prometheus Operator
    âœ”ï¸ CRDs


ğŸ” High Availability Architecture
1ï¸âƒ£ Prometheus HA (Active-Active)
â¤ Two Prometheus instances
â¤ Scrape same targets
â¤ Same alert rules
â¤ Alertmanager deduplicates
ğŸ‘‰ Prometheus has no native clustering.

2ï¸âƒ£ Grafana HA
â¤ Multiple replicas
â¤ Load balancer
â¤ Shared database
ğŸ‘‰ Grafana scales horizontally easily.


ğŸ” Long-Term Storage Architecture
Prometheus alone:
âŒ Short retention
âŒ Single-node
Solution:
    âœ”ï¸ Thanos
    âœ”ï¸ Cortex
    âœ”ï¸ Mimir

Prometheus
   â†“ (remote write)
Long-Term Store
   â†“
Grafana


ğŸ” Federation Architecture
Used for:
    âœ”ï¸ Multi-cluster monitoring
    âœ”ï¸ Hierarchical setups
Leaf Prometheus
        â†“
   Central Prometheus
ğŸ‘‰ Used cautiously due to scaling limits.


ğŸ” Scaling Challenges
1ï¸âƒ£ Cardinality Explosion
    âœ”ï¸ Too many labels
    âœ”ï¸ High memory usage

2ï¸âƒ£ Query Cost
    âœ”ï¸ Long time ranges
    âœ”ï¸ Regex matchers

3ï¸âƒ£ Storage Growth
    âœ”ï¸ High scrape frequency
    âœ”ï¸ Large metric sets
ğŸ‘‰ Architecture decisions directly affect cost & stability.


ğŸ” Security Architecture
â¤ TLS between components
â¤ RBAC in Kubernetes
â¤ Secure /metrics endpoints
â¤ Auth on Grafana
â¤ NetworkPolicies
â¤ Monitoring systems are high-value attack targets.
